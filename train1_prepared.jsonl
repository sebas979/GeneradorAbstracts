{"prompt":"Análisis de algoritmos criptográficos clásicos vs algoritmos cuánticos. ->","completion":" En este trabajo de investigación se realizó una comparación de los algoritmos criptográficos cuánticos vs clásicos con respecto a la seguridad en la obtención de la clave, en base a una revisión sistemática de donde se definió parámetros medibles de comparación, entre estos algoritmos como: la unidad estructural de la información, base de su seguridad, origen de la clave, tamaño de la clave, si el atacante copia la información, detección de intrusos y tecnología aplicada en el mercado, es de importancia conocer el algoritmo criptográfico más fuerte para la seguridad de la información debido a que la tecnología va en incremento y al mismo tiempo la información se convierte en un activo de gran valor a proteger ya que los hackers al interceptar información pueden realizar denegación de servicios, falta de integridad en datos, daños físicos a estructuras, robos millonarios de dólares e información. También se desarrolló el estudio técnico-económico para la de implementar un sistema cuántico en una infraestructura gubernamental del Consejo Nacional Electoral (CNE). Del estudio se determinó que los algoritmos criptográficos cuánticos son más apropiados para este propósito, esto se demostró con el método estadístico CHI cuadrado, para la comprobación de hipótesis. Se concluye que es técnicamente factible la propuesta de implementar criptografía cuántica con el algoritmo BB84 en la estructura nacional gubernamental CNE, con una inversión aproximada al 0.022% del gasto público de 2017. Se recomienda implementar la propuesta técnica económica planteada en la presente investigación para en la infraestructura de CNE en función de garantizar la seguridad de la información.In this research work, a comparison of quantum vs classical cryptographic algorithms was made with regard to the security in obtaining the key, it was based on a systematic review where measurable parameters of comparison were defined, among these algorithms as: the structural unit of information, base of its security, key origin, key size, if the attacker copies the information, intrusion detection and technology applied in the market, it is important to know the strongest cryptographic algorithm for the security of the information because the technology is increasing and at the same time the information becomes a valuable asset to protect since the hackers when intercepting information can perform denial of services, lack of data integrity, physical damage to structures, thefts of millions of dollars and information. Also the technical-economic study was developed to implement a quantum system in a governmental infrastructure of the National Electoral Council (NEC). From the study, it was determined that the quantum cryptographic algorithms are more appropriate for this purpose; this was demonstrated with the CHI square statistical method, for the verification of the hypothesis. It is concluded that the proposal to implement quantum cryptography with the BB84 algorithm in the national government structure NEC is technically feasible, with an approximate investment of 0.022% of public expenditure in 2017. It is recommended to implement the economic technical proposal raised in this research for the infrastructure of NEC in order to guarantee the security of the information.\n"}
{"prompt":"Modelo poblacional con algoritmos genéticos ->","completion":" Para el desarrollo de este trabajo, “MODELO POBLACIONAL CON ALGORITMOS GENÉTICOS”, he investigado la rama de la inteligencia artificial, como son los algoritmos genéticos. Primero presento en forma general los aspectos que envuelven los algoritmos genéticos, parto de la necesidad de optimizar, así como su historia y posibles aplicaciones y luego he cubierto detalladamente todo lo que pude investigar sobre la teoría de los algoritmos genéticos, sus fundamentos matemáticos, tipos de algoritmos genéticos, representación del mismo, partes y operadores más importantes de los algoritmos genéticos. Además se presenta una pequeñísima comparación con otros métodos de programación evolutiva, tales como los algoritmos genéticos paralelos, también llamados algoritmos meméticos, las redes neuronales, y los autómatas celulares. Además hago un estudio de los principales modelos de crecimiento poblacional, aplicados tanto a humanos como a animales y luego propongo un modelo basado en el paradigma de los algoritmos genéticos llevando a cabo una simulación poblacional utilizando una aplicación que he desarrollado completamente en el lenguaje de cuarta generación, Visual C++ 6.0. Finalmente, comparo entre mi modelo de crecimiento poblacional y los modelos convencionales, basándome en un subconjunto de datos de los que son generados por mi modelo, y realizar una predicción de los restantes.\n"}
{"prompt":"Nuevas Tecnologías de computación ->","completion":" Todo nuestro desarrollo en comunicación se ha realizado sin intervención o apoyo gubernamental y, aún sin la asistencia de la UNESCO. Disponer de personal debidamente preparado a cargo del sistema para que la información sea creada, difundida y coordinada en forma efectiva dentro de cada institución. Sin esta coordinción, las computadoras y la comunicación interactiva se convierten en un juguete y un lujo del individuo que tuvo la osadía suficiente como para introducir tales elementos en la institución, pero pasan ser simplemente una pérdida en finanzas y creatividad de aquellos que no comparten ni participan en el uso de su pleno potencial.\n"}
{"prompt":"Mecánica de la computación ->","completion":" Licenciada en Ciencias de la Educación, Mención Informática\n"}
{"prompt":"Programación de los medios ->","completion":" El 75 por ciento de la programación latinoamericana va dirigida hacia el entretenimiento y tiene procedencia en 54 por ciento del extranjero solo el 20 por ciento es producción nacional. Los programas educativo-culturales apenas llegan al 5 por ciento. Lo que más se produce y difunde son las telenovelas.\n"}
{"prompt":"Programación de los medios ->","completion":" El 75 por ciento de la programación latinoamericana va dirigida hacia el entretenimiento. Los programas educativo-culturales apenas llegan al 5 por ciento. Pero están las telenovelas.\n"}
{"prompt":"Simulación de los Procesos Psicrométricos ->","completion":" Los procesos físicos de evaporación y de condensación del agua y viceversa establecen mecanismos de adición y eliminación de humedad, provocando cambios en los diferentes sistemas ecológicos, habitacionales e industriales. Múltiples procesos de acondicionamiento de aire son necesarios en la industria. Estos procesos han sido modelados matemáticamente como operaciones unitarias de humidificación, deshumidificación y secado, los mismos que están en función de varios parámetros psicrométricos como temperatura, humedad relativa, humedad absoluta, presión, y otros. Su cálculo por ecuaciones resulta más o menos complejo, las soluciones gráficas son más prácticas, pero menos precisas, software y simuladores en psicrometría son escasos. Este estudio desarrolla la creación de un modelo computarizado, creado en función de un algoritmo que puede disminuir la complejidad, acelerar la secuencia de operaciones y mejorar la precisión de los cálculos, útil tanto en la industria como en la docencia. El software es basado en el lenguaje Java® para simular casos y métodos de cálculo de procesos de humidificación, deshumidificación, secado y mezclas de aire basado en las ecuaciones básicas de la psicrometría, para un amplio rango de temperaturas y a diferentes alturas sobre el nivel del mar. El modelo ha sido validado para las condiciones Ecuador, mostrando buena correlación entre los resultados calculados manualmente frente a los obtenidos por la simulación.The physical processes of water evaporation and condensation and the corresponding reverse processes, establish mechanisms of addition and\/or removal of moisture, causing changes in different ecological, housing and industrial systems. Multiple air conditioning processes are needed in the industry, these have been modeled mathematically as unit operations of humidification, dehumidification and drying, in turn, they depend on several psychrometric parameters such as temperature, relative humidity, absolute humidity, pressure, etc. Its calculation by equations is more or less complex; graphic solutions are more convenient, but less precise; software and simulation packages in psychrometry are scarce. This paper develops the creation of a computerized model based on an algorithmic tool that can reduce complexity and improve the accuracy of these calculations, useful both for industrial and teaching applications. This software is based on the ®Java language to simulate cases and calculation methods of humidification, dehumidification, drying and mixtures of air based on psychrometric equations. The software can be used for a wide range of temperatures and at different altitudes above sea level. The validation of the model has been performed in Ecuador´s environmental conditions. Results showed good correlation between the results obtained via manual computation and those obtained using the model.\n"}
{"prompt":"Simulación digital de generadores sincrónicos ->","completion":" Se efectua un estudio del generador síncrono, con el cual será posible desarrollar un modelo matemático que simule el comportamiento de la máquina en cualquier estado de operación. En base de ésto se realiza la simulación digital del generador síncrono que esquemáticamente puede ser resumido de la siguiente manera: se realiza un estudio general de las máquinas sincrónicas, analizando el comportamiento del generador en estado estable. Se describen los diversos modelos que pueden ser usados para la simulación de las máquinas sincrónicas, siendo el modelo matemático implementado aquel que toma como variables de estado a los enlaces de flujo, así también se analizan aspectos adicionales a ser considerados en la simulación, tales como la saturación del entrehierro de la máquina.GuayaquilIngeniero en Electricidad Especialización Potencia\n"}
{"prompt":"Inteligencia Artificial, algoritmos y libertad de expresión ->","completion":" La Inteligencia Artificial puede presentarse como un aliado al momento de moderar contenidos violentos o de noticias aparentes, pero su utilizaciónsin intervención humana que contextualice y traduzca adecuadamente la expresión deja abierto el riesgo de que se genere censura previa En la actualidad esto se encuentra en debate dentro del ámbito internacional dado que, al carecer la Inteligencia Artificial de la capacidad para contextualizar lo que modera, se ésta presentando más como una herramienta de censura previa indiscriminada, que como una moderación en busca de proteger la libertad de expresión. Por ello luego de analizar la legislación internacional, informes de organismos internacionales y los términos y condiciones de Twitter y Facebook, sugerimos cinco propuesta tendientes a mejorar la moderación algorítmica de contenidos. En primer término proponemos que los Estados compatibilicen sus legislaciones internas respetando los estándares internacionales de libertad de expresión. También instamos a que desarrollenpolíticas públicas consistentes en implementar legislaciones protectoras de las condiciones laborales de supervisores humanos sobre las decisiones automatizadas de remoción de contenido. Por su parte, entendemos que las redes sociales deben presentar términos y condiciones claros y consistentes, adoptar políticas internas de transparencia y rendición de cuentas acerca de cómo opera la IA en la difusión y remoción de contenido en línea y, finalmente, deben realizar evaluaciones previas de impacto de su IA a los derechos humanos.\/\/ Artificial Intelligence can be presented as an ally when moderating violent content or apparent news, but its use without human intervention that contextualizes and adequately translates the expression leaves open the risk of prior censorship. At present this is under debate within the international arena given that, since Artificial Intelligence lacks the ability to contextualize what it moderates, it is presented more as a tool for indiscriminate prior censorship, than as a moderation in order to protect the freedom of expression. Therefore, after analyzing international legislation, reports from international organizations and the terms and conditions of Twitter and Facebook, we suggest five proposals aimed at improving algorithmic content moderation. In the first place, we propose that the States reconcile their internal laws while respecting international standards of freedom of expression. We also urge that they develop public policies consistent with implementing legislation that protects the working conditions of human supervisors on automated content removal decisions. For its part, we understand that social networks must present clear and consistent terms and conditions, adopt internal policies of transparency and accountability about how AI operates in the dissemination and removal of online content and, finally, they must carry out prior evaluations impact of your AI on human rights.\n"}
{"prompt":"LA RESPONSABILIDAD EXTRACONTRACTUAL Y LA INTELIGENCIA ARTIFICIAL ->","completion":" Al igual que muchas innovaciones a lo largo de la historia, la Inteligencia Artificial está creciendo de forma exponencial, apareciendo cada día en la mayoría de los ámbitos cotidianos, dotando a sistemas y aparatos con la misma. Por consiguiente, el uso de la Inteligencia Artificial generará un volumen de problemas en el ámbito del Derecho, al no existir una normativa nacional e internacional que regule en su totalidad esta nueva tecnología transformadora. El presente trabajo indagará la responsabilidad civil extracontractual, perpetrando un análisis sobre los factores técnicos que provocan dichos problemas jurídicos. Se enfocará en el marco jurídico y fundamentos doctrinales de la responsabilidad extracontractual y se presentarán razones por los cuales se observa la falta de seguridad jurídica de la regulación existente. Lo cual, se ejemplificará mediante ejemplos determinados de posibles daños causados por sistemas dotados de Inteligencia Artificial. Finalmente, se buscará dar soluciones a estos problemas jurídicos, brindando directrices del deber ser de la nueva regulación en materia de daños ocasionados por sistemas dotados de Inteligencia Artificial.Like many innovations throughout history, Artificial Intelligence is growing exponentially, it has appeared among the most commonly things used in daily life, providing most systems, machines, and devices with it. Therefore, the use of Artificial Intelligence will generate a number of problems that the actual national and international legislations around the world does not entirely regulate. Hence this paper will investigate non - contractual civil liability, perpetuating an analysis of the technical factors that cause these legal problems. It will focus on the legal framework and doctrinal institutions of non - contractual liability and will exhibit reasons why the actual legislation does not certainty. This will be exemplified by specific examples of possible damages caused by systems equipped with Artificial Intelligence. Finally, it will seek to provide solutions to these legal problems, providing guidelines on the must be for the new legislations on damages caused by systems equipped with Artificial Intelligence.\n"}
{"prompt":"Creación de un demo de un videojuego con inteligencia artificial ->","completion":" The following paper was born from the search of artificial intelligence used in the video games from programmers and developers, both international and Ecuadorian. In thus, this type of technology has been investigated in this research and, based on an analysis, the results of this investigation can be found below. It proceeds with a historical summary on how the themes of video games and artificial intelligence were born, how they are seen, how they were developed and how they have evolved and innovated over time. The research has been carried out within a world and national perspective. Finally, the main goal with this work is to provide better tools, since it contains an extensive step by step explanation of how to create a video game with artificial intelligence. This document will serve students, programmers and developers as a guide for the creation of their own projects.El presente trabajo es una investigación que nace a partir de la búsqueda de la cantidad de Inteligencia artificial utilizada en los videojuegos de programadores y desarrolladores, tanto ecuatorianos como internacionales. En general, se ha investigado sobre este tipo de tecnologías y a partir de un análisis, a continuación, se podrá encontrar los resultados de dicha investigación. Se procede con un resumen histórico sobre como las temáticas de los videojuegos y la inteligencia artificial nacieron, como son vistos, como se desarrollan y como han ido evolucionando e innovando a lo largo del tiempo. La investigación se ha realizado dentro de una perspectiva de nivel mundial y nacional. Por último, lo que se desea lograr con este trabajo es proporcionar mejores herramientas, ya que el mismo, contiene una extensa explicación paso a paso, de cómo crear un videojuego con Inteligencia artificial. Este documento servirá a estudiantes, programadores y desarrolladores como guía para la creación de sus propios proyectos.\n"}
{"prompt":"Complejidad formal en ladrillo ->","completion":" El presente estudio muestra una recopilaci?n de las diferentes estrategias empleadas para el dise?o y construcci?n de formas complejas que emplean el ladrillo como material principal. Adem?s de esta categorizaci?n tambi?n se analizan las posibles causas que conllevan a la decisi?n de modificar el aparejo de ladrillo com?n.\n"}
{"prompt":"Complejidad, epistemología y multirreferencialidad ->","completion":" Pensamos seriamente en las dificultades para muchos estudiantes de educación de poder explicar y aplicar la epistemología, ya sea porque no la saben, porque poco la conocen o porque quién sabe si podrán creer en sus bondades científicas. A menudo los cursos de epistemología se vuelven densos y teóricos, de difícil entendimiento y en consecuencia los estudiantes poco saben usar sus conceptos y fundamentos. Puede suceder lo que menciona M. Quintana (1983, en Bartomeu, Juárez, Juárez, y Santiago, 1992: 234), que “los epistemólogos no conocen la pedagogía y los pedagogos no saben epistemología”. Nos preguntamos si un estudiante de maestría en prácticas educativas, desarrollo docente, investigación educativa, etc., puede emplear nociones y conceptos de la epistemología con sus alumnos en su tarea docente; es decir, cómo es posible llevar los aprendizajes de la epistemología adquiridos en el aula o de las lecturas de investigación, de las asesorías de tesis, etc., de parte de los estudiantes a un campo de la realidad práctica de su trabajo docente. Deseamos analizar algunos elementos de la “cima” del conocimiento de la epistemología y poderlos llevar al “valle” de la realidad donde se desarrolla la vida académica, sin caer en una falsa epistemología o en una equivocada pedagogía.\n"}
{"prompt":"Globalización y nueva complejidad social ->","completion":" El mundo está atravesando una transición de una sociedad industrial, que giraba en torno del trabajo como fuerza y valor, hacia una sociedad del conocimiento, cuyo núcleo es la información y la capacidad para manejarla y producirla. Esta transformación, que tiende a darse a escala global, ha sido impulsada fundamentalmente por los cambios producidos en el plano de la tecno-economía, cuyos actores primordiales son las transnacionales y los científicos y profesionales de la información. En el plano de la política, que no alcanza a dar respuesta a estos cambios, han cumplido un rol importante los nuevos movimientos sociales, que desplazaron la centralidad de los partidos políticos y de los movimientos sociales clásicos, como los sindicatos.\n"}
{"prompt":"Lógica clásica y lógica difusa: Facetas que las caracterizan ->","completion":" El análisis de información bibliográfica, como proceso metodológico para este artículo, condujo a la elaboración de matrices correspondientes a cada una de las facetas: lógica, relacional, epistémica y de conjuntos, de las lógicas clásica y difusa. La interrelación y análisis de las diferentes premisas constantes en estas matrices permitió la diversificación en la concepción de cada una de ellas, considerando a la lógica clásica como ciencia de los principios formales y normativos del razonamiento, que centra su atención en la forma lógica de adoptar los pensamientos para la construcción de lenguajes formales con claridad y precisión, y a la lógica difusa como ciencia de los principios formales del razonamiento aproximado, flexible y tolerante con la imprecisión, capaz de modelar problemas no lineales.Además, se manifiestan sucintamente aplicaciones de la lógica difusa, entre otras: los sistemas de control y redes neuronales.Palabras clave: Lógica, sistemas de control, redes neuronales.Analysis of bibliographic information, such as methodological process for this article, led to the elaboration of matrices corresponding to each of the facets: logical, relational, epistemic and sets, from classic and diffuse logics. The interrelationship and analysis of different premises constants in these arrays allowed the diversification in the conception of each of them, whereas the classical logic as a science of the formal and regulatory principles of reasoning, which focuses on the logical form of adopting the thoughts to the construction of formal languages with clarity and precision, and diffuse logic as the science of the formal principles of approximate reasoning, flexible and tolerant of imprecision, capable of modeling non-linear problems. In addition, applications of diffuse logic are briefly manifested, among others: control systems and neural networks.Keywords: Logic, control systems, neural networks.\n"}
{"prompt":"Lógica clásica y lógica difusa: Facetas que las caracterizan ->","completion":" El análisis de información bibliográfica, como proceso metodológico para este artículo, condujo a la elaboración de matrices correspondientes a cada una de las facetas: lógica, relacional, epistémica y de conjuntos, de las lógicas clásica y difusa. La interrelación y análisis de las diferentes premisas constantes en estas matrices permitió la diversificación en la concepción de cada una de ellas, considerando a la lógica clásica como ciencia de los principios formales y normativos del razonamiento, que centra su atención en la forma lógica de adoptar los pensamientos para la construcción de lenguajes formales con claridad y precisión, y a la lógica difusa como ciencia de los principios formales del razonamiento aproximado, flexible y tolerante con la imprecisión, capaz de modelar problemas no lineales. Además, se manifiestan sucintamente aplicaciones de la lógica difusa, entre otras: los sistemas de control y redes neuronales.Analysis of bibliographic information, such as methodological process for this article, led to the elaboration of matrices corresponding to each of the facets: logical, relational, epistemic and sets, from classic and diffuse logics. The interrelationship and analysis of different premises constants in these arrays allowed the diversification in the conception of each of them, whereas the classical logic as a science of the formal and regulatory principles of reasoning, which focuses on the logical form of adopting the thoughts to the construction of formal languages with clarity and precision, and diffuse logic as the science of the formal principles of approximate reasoning, flexible and tolerant of imprecision, capable of modeling non-linear problems. In addition, applications of diffuse logic are briefly manifested, among others: control systems and neural networks.Cuencanúmero 2\n"}
{"prompt":"Procedimiento de descarga e indexación de tweets mediante métodos de recuperación de la información ->","completion":" Se investiga un sistema para la obtención e indexación de datos de la red social Twitter basado en Hadoop y la plataforma de búsqueda Solr. Este sistema se diseñó e implementó para el manejo y búsqueda eficiente de vastas cantidades de datos, más en concreto de mensajes cortos o tweets, con la finalidad de alimentar un corpus de texto sobre el cual se pueda realizar operaciones de recuperación de información mediante la plataforma Solr. Hadoop es un framework de computación distribuida de código abierto que ha tomado popularidad en el manejo de big data, que en conjunto al manejo de índices de Solr resultan en un sistema con capacidades de almacenamiento y búsqueda de alto rendimiento.Ingeniero en Sistemas y Telemática\n"}
{"prompt":"Plan maestro de recuperación y fortalecimiento en seguridad de la información para Omnes Ltd. ->","completion":" En el capitulo I, se determinara el marco teórico a seguir para el desarrollo del proyecto, es importante determinar las bases teóricas, base fundamental para el desarrollo de cualquier proyecto que se desea realizar, conocimientos teóricos que permitirán encaminar esfuerzos para el alcance de los objetivos planteados inicialmente. En la actualidad no es suficiente que la empresa se restringa a los campos de higiene y seguridad de trabajo o adquiera una póliza de seguros, como ha sido costumbre. En lugar de soluciones parciales y aisladas, ahora se tiene que buscar soluciones integrales y óptimas para lograr mayor eficiencia dentro de la empresa y prevenir riesgos. Nos hemos referido hasta aquí a la empresa, no sólo a la infraestructura de EDP. Esto se debe a que la empresa es el súper conjunto del centro de cómputo. El procesamiento de datos es una función de apoyo al negocio. Por lo tanto, cualquier tipo de amenaza a la seguridad de la empresa (o cualquier tipo de organización) lo es también para la seguridad de las instalaciones de cómputo. En el capitulo II, el autor de la tesis tuvo como objetivo recolectar toda la información que sea posible, para lo cual se utilizó técnicas ya conocidas para la toma de información como son las entrevista, bibliografía que posee la misma empresa, y la observación de campo, que son principalmente las indicadas y las más idóneas para el desarrollo del proyecto, se encuestaron y entrevistaron a varias personas trabajadores de la empresa para un posterior análisis cualitativo basado en la experiencia y en opiniones de los particulares. Los procesos informales e intuitivos suelen ser más atractivos para la gerencia que los procesos estadísticos y de modelado, la base de las entrevistas y encuestas tuvieron como objetivo, la identificación de registros vitales, procesos y la identificación de las áreas funcionales (departamentos).\n"}
{"prompt":"Configuración de una arquitectura para recuperación de información mediante el uso de una herramienta de Software ->","completion":" Este trabajo se inicia con un análisis general sobre el tema \"Recuperación de Información\", se va a conocer los principales temas que se utilizan en el proceso investigativo.Se diseña una arquitectura de software para el proceso de recuperación de información, la cual posee todas la técnicas y modelos analizados en el proceso investigativo. Todos los componente y subcomponentes trabajan de forma secuencial orientándose al proceso por el pasa la consulta que formula un usuario.This work begins with a general analysis on the topic \"Information Retrieval\", it is going to know the main topics that are used in the investigativo.Ese process is a software architecture for the process of information retrieval, which has all The techniques and models analyzed in the investigative process. All the components and subcomponents work sequentially orienting themselves to the process by passing the query that a user makes.\n"}
{"prompt":"Detección automática de eventos sísmicos en el volcán Cotopaxi mediante técnicas de aprendizaje de máquinas ->","completion":" Los volcanes con un alto riesgo de futuras erupciones pueden causar algún tipo de catástrofe. Con el propósito de preservar vidas humanas y pérdidas materiales, se desarrolló un sistema de detección automática de eventos sísmicos en el volcán Cotopaxi mediante técnicas de aprendizaje de máquinas. Inicialmente, un estudio se realizó de los registros y las etiquetas de señales sísmicas monitorizadas por el Instituto Geofísico de la Escuela Politécnica Nacional (IGEPN) durante el 2012; los datos en formato .SAC y las etiquetas de los eventos sísmicos se extrajeron automáticamente mediante código en MATLAB® para su lectura, concatenación, visualización y posterior procesamiento de la señal. El sistema continúa con el procesamiento digital de señales y segmentado de la señal en ventanas, para cada ventana se extrajeron características en el dominio del tiempo y la frecuencia, que posteriormente sirven para la selección de características relevantes con filtros. Las etiquetas de los eventos se crearon con: etiquetas del IGEPN, técnica STA\/LTA, la unión y las coincidencias de ambos. La detección de eventos sísmicos aplicando Redes Neuronales y SVM se realizaron con la matriz que contiene las características y respectivos etiquetados, se evaluaron mediante diferentes técnicas. Los resultados se presentaron en términos de exactitud, sensibilidad, especificidad, capacidad predictiva y BER, alcanzando porcentajes del 95.35% y 96.92% en exactitud para SVM y Redes Neuronales respectivamente.\n"}
{"prompt":"Predicción de crisis epilépticas utilizando técnicas de procesamiento de señales electroencefalográficas y aprendizaje de máquina ->","completion":" La epilepsia es una enfermedad que se caracteriza por un mal funcionamiento repentino y recurrente del cerebro, denominado \"convulsión\". Las convulsiones epilépticas reflejan los signos clínicos de una actividad excesiva e hipersincrónica de las neuronas en el cerebro, y pueden ir acompañadas de un deterioro o pérdida de la conciencia, síntomas psíquicos, sensoriales, o fenómenos motores. Uno de los aspectos más incapacitantes de la epilepsia es la naturaleza aparentemente impredecible de las convulsiones. Un método capaz de predecir la aparición de crisis epilépticas a partir del electroencefalograma (EEG) de pacientes con epilepsia abriría nuevas posibilidades terapéuticas. Con este enfoque, el presente proyecto contempla el desarrollo de un algoritmo de predicción de crisis epilépticas, usando técnicas de procesamiento de señales electroencefalográficas y aprendizaje de máquina. El algoritmo identifica el estado preictal (pre-crisis), y lo diferencia del estado interictal (sin crisis), basado en los cambios dinámicos espacio-temporales en el registro EEG, que empiezan varios minutos antes de la convulsión. Se emplea una base de datos que consta de registros EEG intracraneales (iEEG) de dos pacientes con epilepsia. Estos registros son analizados en el dominio tiempo-escala empleando la transformada wavelet, y extrayendo una serie de características. Para la clasificación se empleó aprendizaje de máquina supervisado, por medio de los siguientes algoritmos: regresión logística, análisis discriminante, k-vecinos más cercanos (kNN), árboles de decisión, máquina de vectores de soporte (SVM) y conjunto de clasificadores redundantes (ensemble). El algoritmo obtiene una exactitud total de 99.1%, sensibilidad de 99%, especificidad de 99.1% y capacidad predictiva (AUR) de 0.98.\n"}
{"prompt":"Predicción de alertas de incidentes para un servicio integrado de seguridad mediante aprendizaje de máquinas ->","completion":" Un Servicio Integrado de Seguridad se encarga de atender las llamadas de emergencia de la ciudadanía para prestar la debida atención mediante la asignación de los recursos necesarios que requiera el incidente, a través de las instituciones de policía, bomberos, sanidad, militares, salud, etc.; por el hecho de que se debe atender la solicitud de una gran cantidad de emergencias no previstas, hay ocasiones en que no se tienen los recursos listos, resultando en consecuencias fatales. Para que el sistema mencionado pueda ser más eficiente en el aspecto de disponibilidad de recursos y por lo tanto en la atención de emergencias, se implementa un software capaz de predecir el número y el tipo de incidentes que puedan suscitarse en un distrito, mes, día de la semana y turno (horario) específicos, para lo cual se toma como ejemplo el Servicio Integrado de Seguridad ECU 911 y el centro zonal Quito específicamente. Dicho objetivo se realiza mediante la aplicación de aprendizaje de máquinas, exactamente las máquinas de vectores soporte (SVM) de clasificación y regresión, utilizando bases de datos de llamadas de emergencias del año 2014 proporcionada por la empresa. Después de realizar una serie de pruebas de diferentes modelos SVM con distintos métodos de tratamiento de datos y basándose en el mejor rendimiento de acuerdo al MAE y al error relativo, se elige el mejor modelo con respuestas cuantificadas a escala exponencial a 512 niveles y kernel gaussiano, con el cual se crea una interfaz gráfica amigable para manejo de cualquier usuario.\n"}
{"prompt":"Modelado, simulación y control de velocidad de un rotor excéntrico. ->","completion":" El trabajo de titulación consistió en la simulación numérica de un sistema PID (Proporcional-Integrativo-Derivativo) para controlar la velocidad excéntrica de un rotor, cuyo propósito final era presentar el control de un rotor excéntrico. Primero, se aprendieron sobre las propiedades físicas y se estudió el movimiento de tal rotor, luego se enfoca en la fundamentación teórica de operación del sistema de control PID. También, se muestra el curso de la búsqueda numérica para la solución de la ecuación de movimiento del rotor excéntrico. En lugar de realizar experimentos en el rotor real, el análisis se lleva a cabo mediante simulación numérica sobre la plataforma MatLab. La tarea en sí también presenta el proceso de implementación de esta simulación y la obtención de resultados. Finalmente, los resultados también son analizados e interpretados. Se observó cómo el sistema de control responde a los rápidos cambios en la velocidad de rotación del rotor excéntrico.\n"}
{"prompt":"Modelado y simulación dModelado y simulación de reactores CSTR para tratamiento aerobio de aguas residuales domésticas, generadas por las Cdlas. Entre Ríos y Los Arcos ->","completion":" En la actualidad el problema de contaminación ambiental referido al tratamiento de aguas residuales domesticas son generadas por las comunidades organizadas de los países latinoamericanos. El estudio que se realizó determina in situ la recolección para la caracterización de estas aguas a la entrada y la salida de las plantas de tratamiento de aguas residuales domesticas con el fin de analizar la relación de aproximación del diseño del modelado de los diferentes equipos (reactor CSTR, equipo de aireación y oxigenación) y la simulación para obtener cual es el mejor diseño con respecto a las ciudadelas seleccionadas. De acuerdo con la interpretación de los resultados se observa que hay una mayor contaminación en la ciudadela Los Arcos en comparación con la ciudadela Entre Ríos. El parámetro más crítico se analiza en la DBO5, la cual se observa que la ciudadela los arcos presenta mayor contaminación de carga orgánica en un 90.38% lo que podemos inferir que el agua de los arcos presenta mayor contaminación esto se debe a que esta planta de tratamiento no solo trata el agua de los Arcos, sino que trata ciudadelas cercanas, todos los parámetros que se han analizados están por encima del límite de máximo permisible el único parámetro que está cumpliendo el estándar en el pH.\n"}
{"prompt":"Modelado y simulación de la extrusora tipo husillo mediante el uso del software Ansys Cfd Simulation ->","completion":" El objetivo del modelado y simulación de la extrusora tipo husillo mediante el uso del Software ANSYS CFD permitió realizar una simulación en tiempo real del proceso de extrusión del Polipropileno, además de determinar las propiedades del material plástico cuando el mismo es sometido bajo ciertas condiciones. La principal ventaja del uso de este programa es que facilita el desarrollo y solución de problemas de cálculo y diseño, acorta tiempos y permite observar además de verificar el proceso del fluido, el cual pasa por varias etapas entre ellas: la zona de alimentación, plastificación, dosificación y descarga. El diseño del equipo se lo realiza con medidas reales, esto a fin de que los resultados obtenidos se encuentren lo más cercano posible a la realidad. Con el modelo generado de la extrusora se procede a ingresar datos (inputs) del Polipropileno entre los que constan valores de densidad, viscosidad dinámica, Cp y conductividad térmica como parte del fluido, y como parte de equipo se configura el material de construcción propio de la extrusora. Con todos los parámetros ya establecidos se fijan las condiciones de frontera donde se configuran valores de presión ambiental y temperatura de 3 resistencias que alcanzan los 500 grados Celsius. Finalmente se procede a ejecutar el Simulador en primera instancia para el precalentamiento del equipo en donde el fluido netamente es aire, y posterior a esto cuando alcanza la temperatura seteada de 175 grados Celsius de extrusión del Polipropileno se procede a ingresar el nuevo fluido preestablecido del material plastificante para observar su comportamiento en el transcurso del tiempo, cabe mencionar que para alcanzar esta meta se trabajó con un determinado número de iteraciones y tiempo establecidos por el usuario. Las principales variables de análisis proporcionadas por el software al concluir el análisis de fluidos fueron: presión, temperatura y velocidad. Además, se procedió a realizar un análisis del material que entra al equipo el cual posee ciertas propiedades que pueden influir en el proceso de extrusión. Se recomienda considerar para el análisis las pérdidas de calor existentes durante el proceso.The objective of the modeling and simulation of the screw extruder by using the ANSYS CFD software allowed a real-time simulation of the extrusion process of the Polypropylene in addition to determining the properties of the plastic material when it is submitted under certain conditions. The main advantage of the use of this program is that it facilitates the development and solution of calculation and design of problems, shortens times and allows observing as well as verifying the process of the fluid, which goes through several stages such as: the feeding, plasticizing, dosing and unloading area. The design of the equipment is carried out with real measurements so that the results obtained are as close as possible to reality. With the model generated of the extruder, data (inputs) of the Polypropylene are entered, among which are values of density, dynamic viscosity, Cp and thermal conductivity as part of the fluid, and as part of the equipment, the extruder's construction material is configured. With all the parameters already established, the boundary conditions are set where the values of environmental pressure and temperature of 3 resistances reaching 500 degrees Celsius are configured. Finally, the Simulator is run first to preheat the equipment where the fluid is clear air, and then when it reaches the set temperature of 175 degrees Celsius of the polypropylene extrusion, the new pre-established fluid of the plasticizing material is introduced to observe its behavior over time. It is worth mentioning that to achieve this goal, a certain number of iterations and time established by the user were used. The main analysis variables provided by the software at the end of the fluid analysis were: pressure, temperature, and speed. Besides, an analysis of the material entering the equipment was carried out, which has certain properties that can influence the extrusion process. It is recommended to consider for the analysis the heat losses existing during the process.\n"}
{"prompt":"Desarrollo de una interfaz humano-computador mediante la animación de avatares generados a partir de fotogrametría ->","completion":" Este trabajo presenta el desarrollo de una interfaz Humano-Computador. Se muestra el diseño de la aplicación móvil para el entrenamiento emocional, el desarrollo de dos avatares generados a partir de técnicas de fotogrametría, así como el desarrollo del reconocimiento facial. Además de los diferentes softwares y librerías utilizadas para el desarrollo.This work presents the development of a Human-Computer interface. The design of the mobile application for emotional training is shown, the development of two avatars generated from photogrammetry techniques, as well as the development of facial recognition. The different software and libraries used for development are shown.\n"}
{"prompt":"Estudio de la notación ConcurTaskTrees para el análisis de tareas en el desarrollo de la interacción humano-computador y su aplicación en el entorno CTTE ->","completion":" El presente proyecto tiene como objetivo realizar un estudio de la notación ConcurTaskTrees para el análisis de tareas en el desarrollo de la interacción humano-computador y su aplicación en el entorno CTTE (ConcurTaskTrees Environment). Para lo cual se realizó a una investigación detallada en donde se recopiló extractos importantes de artículos técnicos con respecto al tema, los cuales entrarán en un estudio de exclusión con el propósito de obtener información valedera y real. Con el fin de que el proceso de desarrollo de software este centrado en el usuario, se hizo un análisis de tareas de una Banca Virtual para que con los cuyos resultados del mismo se ingresaron en la herramienta CTTE con el objetivo principal de ver visualizar la el modelo de interacción que resulte más factible y amigable para el usuario.\n"}
{"prompt":"Investigación descriptiva de lenguaje basado en modelos \"María\" para el análisis de tareas en el proceso de interacción humano computador y su aplicación en una herramienta CASE ->","completion":" A pesar de los problemas que presentan los sistemas informáticos actuales, estos se han convertido en una herramienta de apoyo en la vida cotidiana de las personas, debido a que intervienen en la mayoría de sus actividades. Algunos de los problemas identificados están asociados a la dificultad de uso del sistema, los cuales generalmente son defectos a nivel de sus modelos de HCI (de las siglas en inglés Human Computer Interaction) (por ejemplo: de interacción, interfaz gráfica, tareas, etc.). Por lo tanto, a un mediano o corto plazo, se precisa de mayor cantidad de recursos para corregir estos errores y mejorar la funcionalidad de los sistemas. El presente estudio se enfoca en el uso de la técnica de Análisis de tareas, que ayuda a obtener una mayor compresión del problema en la fase de Análisis; así como también a realizar de mejor manera el diseño de interacción de la fase de diseño del desarrollo Software. En este contexto, se pueden identificar varias formas para realizar el Modelado de tareas; sin embargo, lo deseable sería utilizar un lenguaje que maximice las características principales del proceso HCI, con la finalidad de que el prototipo resultante se ejecute en varias plataformas y obtener múltiples niveles de abstracción (Xavier, 2003).\n"}
{"prompt":"El pensamiento mitológico como sistema cognitivo de las etnociencias ->","completion":" El tratamiento de diversos conocimientos geométricos, algebraicos o simplemente aritméticos de las sociedades a las que se consideran ágrafas, ha dejado de ser una subdisciplina nacida de la antropología e inserta en las matemáticas y pretende convertirse en una nueva ciencia. Dos inquietudes suscitan esta pretensión: en primer lugar, si el episteme o sistema cognitivo de las etnomatemáticas encuentra soporte válido en el pensamiento mitológico que figura como antecedente y, a la vez, fundamento de las etnociencias; y, en segundo lugar, si las etnomatemáticas admiten una teoría, y si ésta, de ser factible, reuniría los requisitos de una teoría autónoma. Los autores analizan estas inquietudes asumiendo como referente la literatura producida por etnólogos, filósofos y etnomatemáticos que han abordado, desde diversas perspectivas, temas involucrados en este artículo.\n"}
{"prompt":"Sistema de recomendación de asignaturas mediante mapas cognitivos difusos basados en el récord académico del estudiante universitario ->","completion":" Trabajo propuesto para desarrollar un modelo inteligente de recomendación para la orientación curricular de asignaturas en el registro académico de los diferentes niveles de educación superior. Está basado en la combinación de algoritmos de agrupamiento, análisis y simulación de mapas cognitivos difusos; de esta manera se podrá predecir el comportamiento y a su vez el impacto sobre el futuro aprovechamiento académico del estudiante.GuayaquilMagíster en Investigación Matemática\n"}
{"prompt":"Sistema de recomendacion de asignaturas mediante mapas cognitivos difusos basados en el record academico del estudiante universitario ->","completion":" Los sistemas de recomendacion resultan de gran utilidad en la seleccion de alternativas para un individuo, en cualquier contexto. de esta manera se tienen más opciones para tomar una decision importante en beneficio de sus gustos y preferencias y asi mejorar resultados. en los diferentes niveles de formacion de un estudiante universitario, la eleccion de asignaturas se hace compleja, debido a la poca informacion que disponen y la limitada experiencia para tomar decisiones trascendentales en su vida academica.GuayaquilMAESTRÍA EN INVESTIGACIÓN MATEMÁTICA\n"}
{"prompt":"Instalación de sistemas inteligentes para edificios ->","completion":" El presente trabajo se desarrolló con la finalidad de que el Ingeniero Civil y todas las personas involucradas en la construcción, puedan contar con una guía práctica y completa para construir o fiscalizar las instalaciones que en la actualidad son de mucha utilidad y que cada vez serán implementadas en los proyectos de edificios con mayor frecuencia. El Ingeniero Civil, especialmente el Ingeniero de la P.U.C.E. posee un basto conocimiento en lo referente a técnicas y procesos de construcción de los diferentes elementos arquitectónicos y estructurales, no así en lo referente a otro tipo de instalaciones principalmente de sistemas inteligentes, y es por esto que se ve la necesidad de brindar una guía en lo referente a la construcción de este tipo de instalaciones.....\n"}
{"prompt":"Sistema tutorial inteligente ->","completion":" In the teaching-learning process four key elements intervene: the student, the educational one, the information and the means that it surrounds the student. The students are related from early age with the handling of the technology, they find very motivante; for this reason always they are willing to give bigger time to activities that use the computer with the only recompense of using them. In other societies to the daily activity of the students in the class living room, he\/she has been united the interaction with you scheme intelligent, in this relationship, the systems intelligent tutoriales have entered (STI) whose characteristic main they are: to promote an active answer in the student, to inform the acting, to allow an autonomous learning, to promote the efficiency and effectiveness. The STIs the same as the teacher thinks about queries like: what to teach?, when to teach? and how to teach?. Does he\/she think about then if in our society it will Improve the activity of the class the use of a system intelligent tutorial for the learning of the Physics, specifically in the topic of the half speed of the particle? The answer to this query is affirmative, this it is based on the high degree of acceptance and satisfaction of the group in study toward the use of a prototype of system tutorial.\n"}
{"prompt":"Contenedores para jardinería interior basados en sistemas inteligentes ->","completion":" 1. Planteamiento del Problema. --2. Marco Teórico. --3. Metodología. --4. Propuesta. --5. Evaluación Preliminar. --6. Prototipo Virtual y\/o Físico. --7. Conclusiones y Recomendaciones.En la actualidad el hombre urbano vive aislado del entorno natural, pero al ser un ente natural necesita un contacto directo con el entorno natural, es por ello que el objetivo del presente proyecto de investigación es facilitar la creación de espacios verdes dentro de la urbe, aprovechando superficies verticales inutilizadas dentro de su arquitectura. El sistema también pretende hacer el cuidado de las plantas más fácil y automático mediante el uso de sistemas de control que otorguen la autonomía suficiente al equipo, para que el sistema supla de manera automática y más acertada las tareas de cuidado, antes llevadas por el usuario como riego, temperatura e iluminación, procurando así la supervivencia y preservación del jardín. Para ello se realizó una recopilación de información teórica enfocada a los sistemas para el cultivo de jardines en interiores, plantas aptas para este entorno, cuidados de las mismas y materiales utilizados en dichos sistemas. Adicionalmente se aplicó una investigación de campo hacia técnicos e ingenieros electrónicos con base en entrevistas para determinar la arquitectura y componentes de un sistema inteligente de este tipo además de su funcionamiento e integración dentro del producto. Además, se llevó a cabo la experimentación y la observación para determinar dimensiones óptimas para el producto en cuanto al tamaño y morfología de las plantas para que estas tengan un desarrollo normal. De esta forma llegamos a un sistema que protege y preserva el ciclo de vida de una planta dentro de un jardín interior.Pontificia Universidad Católica del Ecuador, Escuela de Diseño IndustrialIngeniero\/a en Diseño Industrial\n"}
{"prompt":"Implementación de un sistema de automatización para el control de semáforos inteligentes ->","completion":" Implementar un sistema de automatización inteligente y control en los semáforos instalados en la intersección de la Avenida 17 de Julio y General José María Córdova de la ciudad de Ibarra, mediante la instalación de un Sensor Loop con la finalidad de mejorar la fluidez del tránsito vehicular.El presente proyecto consiste en desarrollar el estudio e implementar un sistema de automatización para el control de semáforos inteligentes instalados en la ciudad de Ibarra – Imbabura, específicamente en la intersección de la Avenida 17 de Julio y General José María Córdova, el cual permitirá mejorar la fluidez del tránsito vehicular y reducir tiempos muertos en la espera de habilitación de la vía. El diseño que tendrá además una comunicación por medio de una interfaz vía bluetooth para controlar los tiempos de cambio de luces y también modificar el comportamiento de la vía principal o secundaria de acuerdo a las necesidades viales, como por ejemplo en horas pico. El proyecto se lo ha realizado previo a un estudio y análisis de la situación vial actual del sector de la Universidad Técnica del Norte en la intersección de la Avenida 17 de Julio y General José María Córdova, para el cual se tomaron indicadores de tiempo de respuesta de los semáforos instalados, presencia del congestionamiento vehicular y que tan factible han sido ante las necesidades de los conductores en transitar por esta vía. El presente proyecto tiene como objetivo mejorar por completo la fluidez vehicular tanto en la vía principal como secundaria ya que esta es una vía de desfogue de la carretera E35. El diseño del proyecto está basado en la detección de vehículos en la vía secundaria de la intersección “T” mediante un detector de bucle de inducción o Sensor Loop, el cual es controlado mediante Arduino permitiendo generar los tiempos adecuados para la activación de las luces rojo, verde y amarillo, dependiendo del sistema de semaforización. Al final se realiza un análisis de aporte y beneficio para el departamento encargado de transito de la ciudad, Empresa Pública de Movilidad del Norte (MOVILDELNOR) para la implementación de este sistema.\n"}
{"prompt":"Implementación de un Sistema de Almacenamiento Inteligente en Dos Ejes X - Y. Caso Práctico: Laboratorio de Automatización Industrial EIS ->","completion":" Se ha construido un sistema mecatrónico para el almacenamiento inteligente de palets en dos ejes x-y, el mismo que estará implementado en la ESPOCH en el Laboratorio de Automatización Industrial, con la finalidad de optimizar la gestión de almacenamiento e inventario de los mismos. Para el desarrollo se adaptó la metodología de desarrollo de software XP (Xtreme Programming), incluyéndose fases de desarrollo e implementación mecánica, electrónica y neumática. La parte lógica del almacén está conformada por un panel frontal desarrollado en Labview, que facilita al administrador, el control del sistema mecatrónico, además, se diseñó una aplicación web en ASP.NET, facilitando la distribución de productos almacenados, gracias a la implementación de la base de datos en SQL Server 2005. Para el ingreso del palet al sistema, los sensores determinan su tipo, se almacena físicamente y en la base de datos, estableciéndose su estado a disponible, una vez que el palet es extraído, su estado cambia a despachado. El proceso redujo el tiempo de almacenamiento y despacho a menos de 2 minutos, el inventario se lo realiza en menos de 3 segundos, además el personal necesario para su operación es de una persona. Con la implementación del sistema mecatrónico para el almacenamiento inteligente de palets, se optimizó la gestión de almacenamiento e inventarios, lo cual se reflejó en la reducción de costos de operación. Se recomienda para la correcta instalación y utilización del sistema de almacenamiento se apliquen los manuales técnico y de usuario adjuntos a esta tesis.\n"}
{"prompt":"Estudio de los algoritmos de reconocimiento de patrones para la automatización de un semáforo inteligente mediante FPGAs ->","completion":" Se diseñó e implementó un prototipo de semáforo inteligente mediante una tarjeta electrónica de arreglos de compuertas programables en campo (FPGA), para minimizar la congestión vehicular. Para su diseño se empleó LabView 2012 ya que facilita la unión de la visión artificial con la tarjeta electrónica de arreglos de compuertas programables en campo (FPGA); en su implementación se utilizó madera MDF, pistas eléctricas, sensores de contacto, fuente de computadora, cámara, tarjeta electrónica de arreglos de compuertas programables en campo (FPGA) y Arduino. El control automático del semáforo, se realizó con la tarjeta Spartan 3-E, la cual se encarga de procesar mediante el algoritmo de coincidencia de colores la información adquirida por la cámara. Como resultado se obtuvo que en el sistema inteligente el tiempo de espera aproximado para 2 autos fue 18,75 ms, mientras que en el sistema tradicional el tiempo de espera aproximado para la misma cantidad de autos fue 60 ms. Se concluye que el prototipo de semáforo inteligente diseñado e implementado, actúa en función de la cantidad de autos, disminuyendo los tiempos de espera y minimizando la congestión vehicular. Se recomienda probar el prototipo diseñado en condiciones reales.\n"}
{"prompt":"Implementación de un componente interactivo de visualización de estructuras de conocimiento. ->","completion":" En los últimos años, internet se ha ido poblando con una gran cantidad de datos semánticos los cuales han proporcionado conocimiento a la web. Las estructuras semánticas de conceptos relacionados pueden ser aprovechadas para apoyar diferentes tareas académicas o basadas en conocimiento, permitiendo la exploración entre los conceptos de un dominio para conocer las interrelaciones jerárquicas entre los conceptos. El presente proyecto permite la explotación de fuentes de conocimiento con el objetivo de facilitar al usuario la exploración visual a través de estas estructuras, por medio de la herramienta D3SPARQL, la cual es una librería de JavaScript que puede ser incrustada en cualquier lenguaje de programación y permite acceder a datos basados en modelos SKOS o simplemente que tengan una estructura semántica. El resultado del proyecto es un componente interactivo de visualización de estructuras de conocimiento denominado “CIVEC”. En tiempo de ejecución, la solución construye consultas interactivas sobre fuentes de datos RDF, de forma fácil y transparente para el usuario. Es decir, se evita que el usuario común tenga que dominar las complejas tecnologías subyacentes.In recent years, the internet has been populated with a large amount of semantic data which have provided knowledge to the web. Semantic structures of related concepts can be harnessed to support different academic or knowledge-based tasks, allowing the exploration between the concepts of a domain to know the hierarchical interrelations between concepts. The present project allows the exploitation of knowledge sources in order to facilitate the user's visual exploration through these structures, through the tool D3SPARQL, which is a JavaScript library that can be embedded in any programming language and Allows access to data based on SKOS models or simply having a semantic structure. The result of the project is an interactive component of visualization of knowledge structures called \"CIVEC\". At runtime, the solution builds interactive queries over RDF data sources, easily and transparently for the user. That is, it prevents the common user from having to master the complex underlying technologies.\n"}
{"prompt":"Modelos de recomendación basados en conocimiento mediante el empleo de la computación con palabras ->","completion":" El continuo desarrollo de nuevas tecnologías ha llevado al crecimiento acelerado de la información digital. Es cada vez mayor la necesidad de crear alternativas que permitan la restauración y organización de toda la información que se proporcione a los usuarios de manera eficaz. En el siguiente trabajo se propone un sistema de recomendación basado en conocimiento mediante el empleo de la computación con palabras, con fines de mejora en las habilidades de resolución de problemas de los programadores, Se aplicará el método deductivo y la exploración. Utilizando esta metodología se analizara los diferentes tipos de sistemas recomendación para poder desarrollar un sistema de recomendación a partir de la información examinada, como resultado se da a conocer un modelo de recomendación de productos siguiendo el enfoque basado en conocimiento, la cual facilita la construcción de un perfil de usuario y BD, calculando el vector peso utilizando cuantificadores lingüísticos, de tal manera que se obtiene los resultados de la recomendación cuyos valores se acerquen más al perfil de usuario.The continuous development of new technologies has led to the accelerated growth of digital information. There is a growing need to create alternatives that create the restoration and organization of all the information that is provided to users in an efficient way. In the following work, a recommendation system based on knowledge is proposed through the use of word computing, in order to improve the problem-solving skills of programmers. The deductive method and exploration will be applied. Using this methodology, the different types of recommendation systems will be analyzed in order to develop a recommendation system from the information examined, as a result, a product recommendation model is released following the knowledge-based approach, which facilitates the construction of a user profile and DB, calculating the weight vector using linguistic quantifiers, in such a way that the results of the recommendation are obtained whose values are closer to the user profile.\n"}
{"prompt":"Estudio sobre el aprendizaje organizacional y la administración del conocimiento en las empresas basado en el pensamiento sistémico ->","completion":" La presente tesis tiene el carácter de investigación sobre los tópicos de organización, modelos de negocios, procesos, empresa, conocimiento, inteligencia, pensamiento sistémico, etc. Lo que se alcanzo es contar con un modelo para realizar diagnóstico situacional sistémico el mismo que podrá ser aplicado en cualquier tipo de organización pequeña y mediana bien sea del sector público o privado. Las organizaciones donde se aplique ésta herramienta podrán contar con un diagnóstico sistémico actualizado y a tiempo de tal manera que les permita tomar decisiones en el corto, mediano y largo alcance. Las organizaciones pueden ser territoriales, sociales, políticas, educativas, ambientales, etc., es decir es aplicable a todas las empresas que tengan un cierto grado de organización y que por sobre todo quieran cambiar y ser competitivas. La temática principal está basada sobre el aprendizaje organizacional, el pensamiento sistémico, la teoría general de sistemas; teorías que son aplicables al modelo organizacional y a la toma de decisiones. El pensamiento sistémico en una metodología que nos permite ver al mundo como un todo, donde sus partes interactúan entre sí. De igual manera si hacemos un análisis más a detalle podemos decir que las organizaciones, empresas y asociaciones también son sistemas, cada una de ellas a nivel interno están ligados por interacciones invisibles de actos y acciones, que a menudo tardan años en exhibir plenamente sus efectos. Con el uso de las Tecnologías de Información y Comunicación el problema de aprendizaje se agudiza si no sabemos administrar de manera eficiente y dar soluciones rápidas. Es por ello que se está empezando a utilizar principios, estrategias y la práctica de diálogo, y se procura integrarlos a un contexto contemporáneo.Magíster en Gerencia de Sistemas de InformaciónCuenca\n"}
{"prompt":"Sistemas expertos aplicados a las finanzas ->","completion":" Para el desarrollo de la parte inteligente del sistema de razonamiento basados en reglas se utilizo el lenguaje Prolog con encadenamiento hacía atras y para el desarrollo de la parte inteligencte del sistema de razonamiento basado en casos se utilizo el lenguaje de programación Java. El sistema soporte arquitectura Cliente-Servidor para el lado del cliente se utilizaron applets usando la herramienta Visual Age. El sistema recomendará los instrumentos financieros para cada tipo de inversor de acuerdo a sus datos personales de experiencia en inversiones, horizontes de inversión, entre otros.GuayaquilIngeniero en Computación\n"}
{"prompt":"Aplicación de sistemas expertos al análisis de sistemas ->","completion":" La realización de este trabajo tiene como principal objetivo desarrollar una aplicación web que servirá para simular la interacción existente entre los seres vivos de un mismo ecosistema. Otro objetivo a mencionar es utilizar la aplicación como herramienta educativa la cual conseguirá una mayor inmersión de lo que se lograría con los métodos de enseñanzas tradicionales. Para el desarrollo y utilización de nuestro sistema como herramienta de aprendizaje se pondrán énfasis no solo en captar la atención de los usuarios a través de un juego dinámico y entretenido sino de mantenerla, para esto se debe ser empático y ponerse en el lugar del jugador, para que le resulte interesante la aplicación. nuestro trabajo permite emular por medio de una aplicación la habilidad de toma de decisiones mediante relaciones, según las interacciones existentes entre los seres vivos de un ecosistema y las respuestas del usuario durante la ejecución del sistema.GuayaquilIngeniero en Ciencias Computacionales Especialización Sistemas Multimedia \/ Ingeniero en Ciencias Computacionales Especialización Sistemas de Información\n"}
{"prompt":"Aplicación de sistemas expertos al análisis de sistemas ->","completion":" La realización de este trabajo tuvo como principal objetivo desarrollar una herramienta de aprendizaje que simule la interacción de organismos de distintas especies que conviven y compiten por su supervivencia en un ecosistema acuático. Esta herramienta fue diseñada para permitir al usuario a través de un juego, simular las interacciones que ocurren entre organismos vivos y su medio. La aplicación Guerra de los Mundos “Aldea pez” emplea la técnica de árboles de decisión para realizar la toma de decisiones del usuario bajo diferentes escenarios, y su impacto en el ecosistema a lo largo del tiempo. La base de conocimiento del sistema se desarrolló y validó con la participación de un experto en ecología marina. Una vez implementada, la aplicación fue sometida a un set de pruebas con treinta usuarios de diferentes edades, obteniendo un puntaje superior a cuatro en una escala de cinco niveles para el 80% de la muestra, en cuanto a la facilidad de uso de la aplicación.\n"}
{"prompt":"Generación de inferencias elaborativas en la comprensión de textos narrativos ficcionales en niños de 11 - 13 años ->","completion":" La comprensión lectora es un proceso cognitivo complejo, que supone representaciones mentales del texto en diferentes niveles interrelacionados. Desde la psicología del discurso, se han planteado diversos modelos descriptivos y explicativos de la comprensión y se ha destacado el rol fundamental del conocimiento previo y del control cognitivo que el lector desarrolle sobre los mecanismos operantes en la lectura (Kintsch, 1996; Meilán & Vieiro, 2001; Molinari Marotto & Duarte, 2007; Zwaan & Singer, 2003). Uno de los mecanismos más relevantes es la generación de inferencias: operaciones cognitivas de agregado, sustitución, omisión o integración de información, que intervienen en la construcción de las representaciones mentales cuando se intenta comprender lo leído (León, 2003). Si consideramos que leer textos es parte de la rutina diaria de un sujeto desde temprana edad (Zwaan & Singer, 2003), el estudio de la generación de inferencias adquiere una importancia capital en la investigación del proceso de comprensión lectora. Asimismo, debemos contemplar que los niños leen una gran cantidad de textos narrativos ficcionales durante sus años de escolaridad primaria y, en los últimos cursos, en sexto y séptimo grado, es donde los mecanismos de lectura básicos se automatizan y pueden dar lugar a un mayor control sobre operaciones tales como la generación de inferencias. Considerando estos aspectos, nuestro principal propósito fue comprobar la incidencia que un grupo de inferencias, las elaborativas, tiene sobre la comprensión de textos narrativos ficcionales en niños de 11-13 años. Para ello realizamos un experimento en el que 56 niños (28 de sexto grado y 28 de séptimo) de un colegio de la Ciudad Autónoma de Buenos Aires leyeron dos cuentos: uno fantástico y otro realista. Luego de la lectura de cada cuento, respondieron un cuestionario de cinco preguntas, que implicaban la generación de inferencias. Antes de la lectura de los cuentos, se administró un primer cuestionario que tuvo la intención de evaluar la experiencia de lectura general y por géneros. Vistos los resultados, se pudo comprobar que las inferencias elaborativas cumplen un rol fundamental en la construcción de representaciones mentales en la comprensión de textos fantásticos y que su generación está ligada a la experiencia de lectura, al conocimiento previo del género y al control que puedan desarrollar los sujetos sobre dichas operaciones. Dados estos factores y como perspectivas futuras de investigación y aportes a la enseñanza de las Prácticas del Lenguaje, esbozamos ciertas propuestas didácticas propiciadoras de la generación de inferencias en la lectura.\n"}
{"prompt":"El impacto de la industrialización en el proceso de fabricación de productos intermedios de hierro o acero sin alear como mecanismo de sustitución de importaciones en el contexto de cambio de la matriz productiva de la industria siderúrgica del Ecuador ->","completion":" Esta tesis de Maestría tiene la intención de evaluar las implicaciones en la industrialización del proceso de fabricación de productos intermedios de hierro o acero sin alear, como mecanismo de sustitución de importaciones en el contexto de cambio de la Matriz Productiva en uno de los Sectores considerados como estratégicos en el desarrollo del país y base fundamental del proyecto de cambio como es la Metal Mecánica. El resultado de este trabajo de investigación es una serie de conclusiones relevantes sobre los antecedentes del proceso de industrialización y sus repercusiones en los volúmenes de importación tanto en unidades comerciales como unidades monetarias durante el período de análisis, así como una serie de inferencias generadas y valoradas en el corto y mediano plazo, con la finalidad de establecer oportunidades y nuevos desafíos en el fomento al cambio de modelo productivo sin enfrentarse a especulaciones o variaciones de mercado. La parte final de este trabajo propone algunas conclusiones y recomendaciones obtenidas en base a la información estudiada y generada durante la investigación.This master's thesis aims to assess the implications of the industrialization of the process of manufacture of steel billets products like mechanism of replacement from imports in the context of changing the Productive Matrix in one of the industrial segments considered like strategic in the development of the country and extremely important in the project of this change as the Metal Mechanics is. The results of this research work are some important conclusions about the antecedents of the process of industrialization and its impact on import volumes both in commercial units as units of currency during the analysis period, as well as a number of inferences generated and valued in the short and medium term, with the purpose of establishing opportunities and new challenges in the promotion to the change of productive model without confronting to speculations or market variations. The final part of this paper proposes some conclusions and recommendations obtained on the basis of the information analyzed and generated during the research.\n"}
{"prompt":"Estudio del comportamiento del DBO en humedal artificial para tratar agua residual proveniente de baños, lavadoras y fregaderos ->","completion":" The present research project related to the topic \"Study of the DBO treatment in Artificial Wetland to treat wastewater coming from bathrooms, washing machines and sinks\", has the fundamental objective of evaluating the removal of the BOD indicator parameter in an artificial wetland. To fulfill the objective of starting the session with the open part, taking into account the concepts issued by various authors on the treatment of wastewater from bathrooms, washing machines and scrubbers and artificial wetlands, types of elements, advantages, parameters, explaining types of wetlands, components, removal mechanisms, and design. Later, please describe the existing treatment system, the existence of the improvements, the implementation of improvements, the realization of the change of the bed, the painted structure, among others, making laboratory tests for the determination of BOD, the type of samples, range of work, methods, quality manual (PG-01), in safety techniques, equipment, materials, reagents, previous operations, procedure, application of formulas, report of results, precision, and records……….El presente proyecto de investigación relacionado con el tema “Estudio del Comportamiento del DBO en Humedal Artificial para tratar Agua Residual proveniente de baños, lavadoras y fregaderos”, tiene como objetivo fundamental de evaluar la remoción del parámetro indicador DBO en un humedal artificial. Para cumplir con el objetivo se inicia con la parte teórica, tomando en cuenta los conceptos emitidos por varios autores sobre el tratamiento de aguas residuales provenientes de baños, lavadoras, fregaderos y humedales artificiales, detallando los tipos, elementos, ventajas, parámetros, explicando sobre tipos de humedales, componentes, mecanismos de remoción, y diseño. Posteriormente, se describe el sistema de tratamiento existente previo a mejoras, en la implementación de mejoras se realiza el cambio del lecho, adecentamiento, entre otros, haciendo pruebas de laboratorio para la determinar DBO, detallando el tipo de muestras, rango de trabajo, métodos, manual de calidad (PG-01), inferencias (sustancias tóxicas), principios (cinco días), medidas de seguridad, equipos, materiales, reactivos, operaciones previas, procedimiento, aplicación de fórmulas, reporte de resultados, precisión, y registros………..\n"}
{"prompt":"EDUCACIÓN A DISTANCIA Y CALIDAD ÉTICA ->","completion":" Este artículo parte de la conceptualización de la ética en la educación para realizar observaciones sobre el compromiso de la educación del estudiante y la sociedad. ¿Moral y ética, ética y moral conducen a un mismo fin? ¿La libertad de elegir es el camino de la ética? ¿Dónde radica la verdadera ética, en el individuo que creció con una educación en valores y procede por convicción o en el que, en base de controles, recompensas y castigos logra un proceder apegado a la ética? Se establece además el compromiso de la educación a distancia con los principios y postulados de las Constituciones de los países Latinoamericanos, relacionados con la educación de calidad y con los costos que implica acceder a los programas de quienes ofertan esta modalidad de estudios. Un acápite importante es lo referente con la corresponsabilidad ética del tutor-estudiante, toda vez que son los actores directos del proceso pedagógico. ¿Están preparados los estudiantes para acceder a lo que implica ser parte de este tipo de educación que se fortalece en el uso de los medios tecnológicos modernos de comunicación, dando lugar a los estudiantes virtuales? ¿Tienen las destrezas en comunicación oral y escrita que el modelo implica? ¿Los maestros están dispuestos a realizar el proceso de seguimiento, motivación, orientación, acompañamiento, la universidad o centro de estudio remunera a sus profesionales a fin de exigirles un rendimiento de calidad? En base a estas reflexiones, es pertinente a realizar ciertas sugerencias en pro de la educación a distancia, que cada vez se fortalece más en el Ecuador\n"}
{"prompt":"Diagnóstico situacional de la carrera educación inicial modalidad a distancia ->","completion":" Introducción. El problema de la investigacón. Marco referencial teórico legal y conceptual. Marco metodológico. Análisis e interpretación de los resultados. Conclusiones y recomendaciones\n"}
{"prompt":"Educación a distancia en el nuevo entorno tecnocultural ->","completion":" El sistema de \"servicios globales de comunicación multimedial\"\" capaz de transformar el significado de la educación particularmente a distancia. En este artículo se señalan las características e implicaciones de este sistema en lo atinente a la educación a distancia\"\n"}
{"prompt":"Diseño de un algoritmo predictivo para el análisis de disponibilidad de canales y el uso eficiente de sistemas con acceso oportunista al espectro basado en el modelo k-vecinos más cercanos ->","completion":" Este estudio se basó en un análisis en la banda wifi de 2.4GHz con datos reales tomados en un transcurso de 10 días, dentro de un edificio en tres diferentes pisos, 1, 9 y 16 respectivamente, ubicado en una zona urbana. Se convirtieron los datos de potencia a datos binarios, obteniendo los canales disponibles y no disponibles para así realizar la agrupación de n canales conjuntos. El método K-vecinos más cercanos, considerado un método de aprendizaje que clasifica los datos recordando los datos anteriores llamados entrenamiento, cada vez que un nuevo dato se presenta al sistema, este lo clasifica de acuerdo al comportamiento del dato más cercano. La cercanía de los vecinos se define en base a los atributos del dato y de los de entrenamiento, el algoritmo calcula la distancia entre los atributos.GuayaquilIngeniero en Electrónica y Telecomunicaciones\n"}
{"prompt":"Estudio de la percepción del umbral de olor mediante relaciones cuantitativas estructura -propiedad ->","completion":" Este trabajo desarrolló un modelo predictivo in silico para el umbral de olor de 176 compuestos orgánicos volátiles (VOCs) basado en una relación cuantitativa estructura-propiedad (QSPR). Las moléculas se optimizaron mediante el método semiempírico PM3 para calcular 5440 descriptores moleculares en el programa alvaDesc. Inicialmente, se usó el método V-WSP para la reducción de descriptores. Posteriormente, el conjunto de datos se dividió en grupos de calibración (70%) y predicción (30%). Los compuestos se dividieron en moléculas de alto y bajo poder odorante para modelarlos mediante el método de clasificación kNN (k-vecinos más cercanos) acoplado con los algoritmos genéticos (GAs). Se obtuvo un modelo con 2 descriptores y 5 vecinos cercanos, utilizando tasa de aciertos en calibración (NERcal = 0.75), validación cruzada (NERcv = 0.75) y predicción (NERpred = 0.78). El modelo se desarrolló con los principios de la Organización para la Cooperación y el Desarrollo Económico (OCDE) para hacerlo aplicable.Ingeniero en Alimentos\n"}
{"prompt":"Clasificación vía aprendizaje automático de conformaciones moleculares en estructuras teloméricas ->","completion":" Un gran número de estudios han sido publicados en el área de pequeñas moléculas unidas a secuencias teloméricas que se pliegan en estructura no convencionales de AD denominadas estructuras G- Cuádruple. El estudio de estas estructuras ha sido motivado por su potencial utilidad como blancos para desarrollo de fármacos. Además, las estructuras G- Cuádruples han sido identificadas en otras secuencias de importancia a lo largo de todo el genoma. Dentro de los diversos estudios se han utilizado dos modelos: la secuencia telomérica humana y las secuencia telomérica el Oxytricha, Tetrahymena. Ambos han sido empleados como modelos para estudios computacionales y ensayos in vitro. Aquí estudiamos la trayectoria generada a partir de las simulaciones de dinámica molecular atomística para obtener la cantidad de flexibilidad y movilidad de estas estructuras modelos. Utilizamos herramientas de clasificación (es decir, método del vecino cercano k) y minería de datos. Encontramos las conformaciones más pobladas y discutimos los hallazgos estructurales.A great number of studies have been published on the area of small molecules bound to telomeric sequences that fold on a non-cannonical DNA structure known as G-quadruplex structure. The study of these structures has been driven for its potential as targets for drug development. Furthermore, G-quadruplex structures have been identified in other sequences of importance through all the genome. Within the different studies two main models have been used: the human telomeric sequence and the oxytricha, tetrahymena telomeric sequence. Both cases are used as models for computational studies and in vitro essays. Here we study the trajectory generated from an atomistic molecular dynamic simulation to obtain the amount of flexibility and mobility of these model structures. We used data mining and classification tools (i.e. k nearest neighbor method) to identify automatically subpopulations of structures within the simulation. We found the most populated conformations and we discuss the structural findings.Cuencanúmero especial\n"}
{"prompt":"Sistemas detector de actitudes negativas en entornos virtuales de aprendizaje. ->","completion":" PDFEn el ámbito académico la presente investigación propone por una parte, analizar y determinar las actitudes negativas que se generan al iniciar una nueva clase y calificar el grado de aceptación de los estudiantes como también las distintas herramientas que ofrecen los modelos educativos, es decir que las actitudes negativas de aprendizaje de los estudiantes sean detectadas a tiempo y reciban observaciones oportunas sobre su funcionamiento y su progreso hacia la realización de los objetivos del curso, para lograr este propósito se ha desarrollado un sistema detector de actitudes negativas en entornos virtuales de aprendizaje el cual analiza los comentarios emitidos de los estudiantes en referencia a cada clase y los almacena en una base de datos recibiendo así un mejor enfoque para el mejoramiento académico del estudiante, es importante. Acorde a los objetivos expuestos anteriormente se puede concluir que se elaboró una revisión bibliográfica sobre los sistemas detectores de actitudes negativas y algoritmos de procesamiento de lenguaje natural para definir los límites y atributos del proyecto, como también la importancia que el sistemas detector de actitudes negativas en entornos virtuales de aprendizaje sea acoplado a la base de datos de la universidad de Guayaquil para poder nutrir más el algoritmo y por ende el resultado sea preciso.In the academic field, this research proposes, on the one hand, to analyze and determine the negative attitudes that are generated when starting a new class and qualify the degree of acceptance of the students as well as the different tools offered by educational models, that is, the Negative learning attitudes of students are detected in time and receive timely observations about their operation and their progress towards achieving the objectives of the course, to achieve this purpose a system has been developed for detecting negative attitudes in virtual learning environments which analyzes the comments issued by the students in reference to each class and stores them in a database thus receiving a better approach for the academic improvement of the student, it is important. In accordance with the objectives set out above, it can be concluded that a bibliographic review was made on negative attitude detection systems and natural language processing algorithms to define the limits and attributes of the project, as well as the importance of negative attitude detection systems in virtual learning environments is coupled to the database of the University of Guayaquil to further nurture the algorithm and therefore the result is accurate.\n"}
{"prompt":"Sistemas de aprendizaje colaborativo m?vil en realidad aumentada ->","completion":" La realidad aumentada educativa es una tecnolog?a que actualmente est? mejorando la calidad de ense?anza, la utilizaci?n de dispositivos m?viles permite que el estudiante sea protagonista de su aprendizaje sin estar confinado a un espacio o tiempo espec?fico para aprender. Aplicaciones colaborativas con realidad aumentada est?n siendo empleadas cada vez m?s en la educaci?n, de tal forma que fomentan el trabajo en grupo donde los estudiantes comparten conocimiento, dudas, opiniones logrando un mejor nivel cognitivo que trabajando individualmente. En este trabajo se presenta el estado de la cuesti?n de Aplicaciones Educativas con Realidad Aumentada en dispositivos m?viles, y Aplicaciones Educativas colaborativas con Realidad Aumentada, desarrolladas desde el 2002 e implementadas en instituciones educativas. As? mismo se realiza un estudio sobre la Realidad Aumentada, Realidad Aumentada m?vil y Aprendizaje M?vil. Adem?s, a partir de las caracter?sticas del estudio de las aplicaciones con Realidad Aumenta, se realiza un an?lisis y dise?o de una Aplicaci?n M?vil para el proyecto de inicio de los alumnos de nuevo ingreso de la UPM. As? como tambi?n una herramienta de autor?a para las gestiones de las actividades propuestas por los docentes de la UPM. Finalmente se presenta un caso de prueba en el que se implementa parte de la propuesta de este trabajo, logrando construir un parte funcional para el proyecto inicial denominado PIANI ? UPM.\n"}
{"prompt":"Sistemas virtuales informáticos en los procesos de enseñanza aprendizaje ->","completion":" PDFEl propósito de esta investigación es evaluar los procesos pedagógicos en las que se encuentran los aprendizajes en la asignatura de computación, en los estudiantes de noveno año de educación básica y diseñar un entorno virtual que contenga técnicas y herramientas tecnológicas. En los antecedentes de estudio la investigación no guarda semejanza con otros trabajos planteados sin embargo existe una amplia bibliografía de temas similares. El contenido del marco teórico está centrado al uso de los sistemas virtuales, los cuales posibilitan el acceso remoto tanto a estudiantes como a profesores desde cualquier lugar y en cualquier momento, mediante una conexión de TCP\/IP (Internet),La filosofía planteada por Moodle incluye una aproximación constructiva basada en el constructivismo social de la educación, enfatizando que los educandos puedan contribuir a la experiencia educativa mediante el uso de las herramientas tecnológicas. Se lo considera como un proyecto factible: entre sus fases abarca una investigación descriptiva y explicativa, además incluye una investigación documental y de campo. Como instrumento de investigación se utilizó la observación “in situ”, a través de entrevistas, encuestas, aplicadas mediante muestreo aleatorio, especialmente en los segmentos docentes y estudiantes de la población en estudio correspondiente a 557 discentes y 60 docentes. De los cuales se tomó como muestra a 58 educandos y 38 maestros, Los recursos que se han utilizado en el presente estudio son: Humanos, técnicos y financieros, El aporte de la tesis es lograr el mejoramiento de la gestión del estudiante con el objetivo de preparar y enviar a la sociedad educandos capaces de enfrentarse a cualquier circunstancia en la vida diaria. La propuesta planteada, es una de esas herramientas, que apoya al docente en su labor educativa y al discente para que aprenda de una manera significativa, y a la vez pedagógica, práctica y dinámica, porque es un sistema virtual interactivo que contiene: textos, sonidos y animaciones, características trascendentales en el aprendizaje actual, Los beneficiarios de este proyecto son, los estudiantes, maestros de la Unidad Educativa el “El Ateneo”. Y la comunidad en general.The purpose of this investigation is to evaluate the computing pedagogical processes of the ninth grade students of basic education and design a virtual environment containing technical and technological tools. In the study antecedents, the investigation is not related to their works mention edbut there is extensive literature of similar topics. It is considered as a feasible project, between its phases covers a descriptive and explanatory research, also includes a documentary and field research. As a research instrument was used observation \"in situ\", through interviews, surveys, applied by random sampling, especially teachers and students segments of the population under study. The sample will be stratified and includes 85 students and 38 teachers, the resources that have been used in this study are: human, technical and financial, the contribution of the thesis is to improve the management of the students in the society , in order to prepare and send them to society and students will be able to face any situation in daily life. The proposal presented, is one of those tools, which supports teachers in their educational work and the learner to learn in a significant way, while teaching, practical and dynamic, because it is an interactive virtual system containing: text, sound and animations, learning transcendental features current beneficiaries of this project are students, teachers of the Education Unit \"El Ateneo\". And the community in general\n"}
{"prompt":"Sistema de reconocimiento de microterremotos en tiempo real del volcán Cotopaxi aplicando aprendizaje supervisado ->","completion":" Vivimos en un mundo continuamente cambiante, donde se genera una variedad de fenómenos físicos naturales. Muchos de estos fenómenos físicos han sido monitorizados constantemente por el peligro latente que generan. Los volcanes son un claro ejemplo de un fenómeno natural que puede traer consecuencias catastróficas si entra en etapa eruptiva. En este contexto, el Ecuador posee uno de los volcanes más activos del mundo, lo que genera la necesidad de monitorizar y recopilar información por expertos en geofísica, los cuales están interesados en entender el comportamiento de dicho fenómeno, por estas razones generan la necesidad de estudiar métodos que permitan identificar posibles erupciones mediante la monitorización y detección de la actividad microsísmica de un volcán, con el fin de salvaguardar vidas y pérdidas materiales. En este proyecto se desarrolló un sistema de reconocimiento automático de microterremotos en tiempo real del volcán Cotopaxi al aplicar técnicas de aprendizaje supervisado. El modelo de reconocimiento es obtenido aplicando modelos de aprendizaje supervisado k-Vecinos Cercanos (kNN), Máquina de Vectores Soporte (SVM) y Árboles de Decisión (DT) a características de tiempo, frecuencia y escala, aplicados a los datos proporcionados por el Instituto Geofísico de la Escuela Politécnica Nacional con la actividad sísmica presentada del volcán Cotopaxi en los años 2012, 2013 y 2014. El sistema de reconocimiento está constituido por una etapa de detección y clasificación, para lo cual se realizan procesos de segmentación de la señal en ventanas, etiquetamiento, extracción de características, selección de características y obtención de modelos unificados en un sistema de votación. Para las etapas de detección y clasificación de microterremotos presentaron resultados en términos de exactitud, precisión, sensibilidad, especificidad y tasa de error de balanceo (BER, del inglés Balanced Error Rate), para la detección el algoritmo kNN alcanzó porcentajes del 98.15 %, 95.20 % y 0.017 para la exactitud, precisión y BER, respectivamente, mientras que en la clasificación el algoritmo SVM alcanzó porcentajes del 94.49 %, 60.42 % y 0.16 para la exactitud, precisión y BER respectivamente.\n"}
{"prompt":"Clasificación de aplicaciones móviles para personas con discapacidad mediante modelos de aprendizaje supervisado ->","completion":" Los dispositivos móviles son actualmente la industria de más rápido crecimiento, ha permitido el desarrollo de aplicaciones móviles acordes a las necesidades de los usuarios, aun así, existe un grupo vulnerable que son las personas con discapacidad que no pueden acceder a las mismas aplicaciones por sus limitaciones. Además, no hay evidencia de que haya una calificación inclusiva como se demuestra en la App Store: Android Google Play y para iOS App Store. El objetivo de este proyecto es clasificar aplicaciones móviles mediante el uso de modelos de aprendizaje supervisado para determinar las aplicaciones inclusivas disponibles en Google Play. La metodología aplicada al proyecto es CRISP-DM son las siguientes fases: Fase 1 - Entendimiento del negocio: Comprender el desarrollo de las tecnologías aplicaciones para las personas con discapacidad, definición de palabras claves enfocadas a cada discapacidad existente. Fase 2 - Entendimiento de los datos: fuente de información se tomó la página de Google Play realizando la búsqueda con las palabras claves. Fase 3 - Preparación de los datos: Se lleva a cabo el proceso de minería de texto aplicada a la descripción de las aplicaciones obtenida de la fase 2. Fase 4 - Desarrollo y evaluación de los modelos: proceso de entrenamiento y test, se valida los modelos de acuerdo a los parámetros de medición: precision, recall, f1-score, accuracy, para los modelos : Naive Bayes, Máquina de vector de soporte lineal, Regresión logística, Nearest-neighbor. La evaluación de los modelos permitió obtener los mejores resultados en cuanto a métricas de precision, recall, f1-score al modelo de máquina de vector de soporte lineal. El modelo Máquina de vector de soporte lineal propuesto para la clasificación de aplicaciones móviles para personas con discapacidad obtuvo los mejores resultados clasificando correctamente en un 70%.\n"}
{"prompt":"Segmentación geométrica de la ciudad de Quito por medio de un método de aprendizaje no supervisado ->","completion":" This document contains an application of data analysis methods on urbanism and Geographic Information Systems. It aims to find shape and density patterns in the map of Quito, starting from the assumption that there are some similarities between the city blocks in terms of its geometric features and its spatial location. The Hierarchical Agglomerative and DBSCAN algorithms have been considered as analysis tools. Since the result is a new partition of the city, in which the new neighborhoods or sectors are not defined in an arbitrary way, we open the door for rethinking urban planning.El presente documento recoge una aplicación de métodos de análisis de datos en urbanismo y Sistemas de Información Geográfica. Se apunta a encontrar patrones de forma y densidad en el mapa de Quito, partiendo del supuesto de que existen similitudes entre las manzanas de la urbe en términos de sus atributos geométricos y su ubicación espacial. Los algoritmos Jerárquico Aglomerativo y DBSCAN han sido considerados como herramientas de análisis. Siendo el resultado una nueva segmentación de la ciudad, en la cual los nuevos barrios o sectores no están definidos de manera arbitraria, se abre la puerta a un replanteamiento de la planificación urbana.\n"}
{"prompt":"Una revisión del aprendizaje profundo aplicado a la ciberseguridad ->","completion":" Este estudio presenta una descripción general sobre la ciberseguridad desde la perspectiva de las redes neuronales y técnicas de aprendizaje profundo de acuerdo con las diversas necesidades actuales en ambientes de seguridad informática. Se discute la aplicabilidad de estas técnicas en diversos trabajos de ciberseguridad, como detección de intrusos, identificación de malware o botnets, phishing, predicción de ciberataques, denegación de servicio, ciberanomalías, entre otros. Para este estudio se aplicó el método analítico-sintético que sirvió para identificar soluciones óptimas en el campo de la ciberseguridad. Los resultados destacan y recomiendan algoritmos aplicables a la seguridad cibernética como base de conocimiento y facilidad para investigaciones futuras dentro del alcance de este estudio en el campo. Esta investigación sirve como punto de referencia y guía para la academia y los profesionales de las industrias de la seguridad cibernética desde el punto de vista del aprendizaje profundo.\n"}
{"prompt":"Clasificación de sílabos académicos en base a redes neuronales de aprendizaje profundo ->","completion":" A fin de facilitar la homologación de créditos entre las universidades del país, se requiere automatizar la clasificación de sílabos en áreas y subáreas. Esta investigación tiene como objetivo clasificar los sílabos mediante la técnica de redes neuronales conectadas de aprendizaje profundo, a través de una combinación de número de capas, funciones de activación, tamaños y épocas de entrenamiento. Este modelo fue comparado con respecto a algoritmos basados en Support Vector Machine (SVM), Naive Bayes y Árboles de decisión. Los resultados demostraron que el modelo de aprendizaje profundo propuesto fue superior en 1.4% con respecto a Naive Bayes, 6.2% con respecto a SVM y 7.2% con respecto a Árboles de decisiónIngeniero en Sistemas y Telemática\n"}
{"prompt":"Desarrollo de controladores con redes neuronales de aprendizaje profundo aplicando tensorflow ->","completion":" En el presente proyecto se desarrollaron algoritmos de redes neuronales usando la librería TensorFlow, para la solución de aplicaciones básicas como el problema XOR, el cual no es separable linealmente, la identificación de una función sencilla (z=x^2+y^2). Por medio de las redes neuronales que se pueden entrenar usando esta herramienta también se realiza la identificación, control neuronal inverso y control neuronal con modelo de referencia lineal de sistemas dinámicos (Antena con péndulo invertido, Tanques Acoplados, Viga y Bola). Las redes neuronales fueron diseñadas y entrenadas en el lenguaje de Python y luego exportados a MATLAB R2017b. Las redes se han entrenado usando aprendizaje supervisado e implementando varias funciones de activación en cada una de sus capas, principalmente “ReLU”. Para el desarrollo y para la trasferencia entre Python y MATLAB del modelo entrenado, se usa una API (Application Programming Interface) la cual es Keras, y Deep Learning Toolbox Importer for Tensorflow-Keras Models. Para la simulación de los modelos entrenados se usa Simulink de MATLAB.\n"}
{"prompt":"Comparación de métodos de reducción de dimensionalidad enfocados a algoritmos de clasificación supervisados aplicado a datos de redes de sensores ->","completion":" Desarrollar una comparación entre los diferentes métodos de RD enfocados a algoritmos de clasificación supervisados, aplicados a datos de redes de sensores con el fin de determinar el más adecuado.El presente proyecto muestra la aplicación de métodos de reducción de dimensionalidad y algoritmos de clasificación con datos obtenidos de cinco distintos sistemas embebidos que hacen uso de redes de sensores inalámbricos con el objetivo de realizar una representación gráfica de los resultados y conjuntamente determinar el más adecuado mediante el cálculo de su eficiencia. Para su desarrollo se realiza el análisis de características de los sets de datos y la preparación de estos, que implica su limpieza y acondicionamiento. Además, un breve estudio para determinar la plataforma correcta que permita trabajar con los datos analizados previamente. Se implementa cada método de reducción de dimensionalidad seleccionado para este trabajo a cada set de datos y consecutivamente se aplica los algoritmos de clasificación a los datos adquiridos. En la parte final como resultados relevantes, se observan los métodos de RD de forma gráfica al ser aplicados a las bases de datos especificadas con anterioridad. Con ello, se realiza una comparación de los datos obtenidos del cálculo de la matriz de confusión para determinar la eficiencia lograda por cada algoritmo dentro de cada método de reducción de dimensionalidad.Ingeniería\n"}
{"prompt":"Extracción de características para la clasificación de imágenes a través de la reducción de dimensionalidad aplicando Análisis de Componentes Independientes (ICA) ->","completion":" En la investigación presentada mediante la tesis \"Extracción de características para la clasificación de imágenes a través de la reducción de dimensionalidad aplicando análisis de componentes independientes (ICA)\" se ha incluido un interesante estudio de lo relacionado con el manejo de los componentes independientes en el área de la visión por computador. Se realizaran varios experimentos con el afán de buscar la mejor técnica para encontrar similitudes entre componentes independientes de distintas imágenes, a fin de comparar una imagen que pertenezca a un grupo de imágenes de pruebas con cada imagen de un grupo de datos de entrenamiento y determinar cuáles imágenes se parecen más entre sí, formando pares de imágenes en base de los cuales calcularemos porcentajes de precisión de los distintos experimentos.This research presented by thesis \"Extraction feature´s for images classifications via dimensionality reduction using independent components analysis (ICA)\" has included a very interesting study of the management of independent components on computer vision area. Several experiments were done to search the best technique to find similarities between independent components of distinct images, what is sought is to compare an image that belongs a group of test images with each image´s data training group and determine which are more similar each other, doing images pairs in basis of which we calculate accuracy percentages of the distinct experiments.\n"}
{"prompt":"Diseño y desarrollo de un sistema de recomendación basado en filtrado colaborativo utilizando datos secuenciales mediante redes neuronales recurrentes ->","completion":" Actualmente los sistemas de recomendación buscan adaptarse a los gustos actuales de los usuarios, siendo las redes neuronales una solución a este problema. En este punto, nuestro trabajo consiste en diseñar y desarrollar un sistema de recomendación basado en filtrado colaborativo utilizando datos secuenciales mediante redes neuronales recurrentes.Currently, recommendation systems seek to adapt to the current tastes of users, with neural networks being a solution to this problem. At this point, our work consists of designing and developing a recommendation system based on collaborative filtering using sequential data through recurrent neural networks.\n"}
{"prompt":"Aplicación de métodos estadísticos para el análisis y predicción de perfiles de consumo de energía eléctrica ->","completion":" El objeto del proyecto investigativo está enfocado en predecir y analizar los perfiles de consumo de energía eléctrica mediante la aplicación de métodos estadísticos, para ello se dispone de información referente al consumo de energía como también de mediciones para variables climáticas.The object of the research project is focused on predicting and analizing the consumption profiles of electric energy through statistical methods. For this, information is provided about energy consumption as well as measures for weather variables.\n"}
{"prompt":"Pronòstico de venta: comparaciòn de la precicion de la predicciòn con diferentes mètodos ->","completion":" Se presentan antecedentes historicos del desarrollo de las redes neuronales, asi tambien como sus primeras aplicaciones. se presenta la teoria matematica que da forma a las redes neuronales. se describe la teoria convencional utilizada para la predicciòn de los valores de una serie de tiempo. se utilizan tres series de tiempo distintas de ventas para realizar la comparacion practica, delos mètodos convencionales y las redes neuronales.GuayaquilIngeniería en Estadística Informática\n"}
{"prompt":"Modelo predictivo del proceso de deshidratación de banano ->","completion":" El presente trabajo consiste en representar lo que sucede en un deshidratador destinado para banano en un modelo matemático y físico, explica y considera fenómenos físicos importantes como son los modos de transferencia de calor que se den en estado transiente y la transferencia de masa en el mismo estado. Se empieza el modelo manteniendo la misma relación del porcentaje de volumen de espacio vacío, para que aplicando uno de los métodos de estado transiente que relacione íntimamente la transferencia de masa y calor, con la finalidad de proceder mediante una corrida computacional aplicándolo primero en sentido de desplazamiento del flujo de aire para que posteriormente se lo haga considerando al tiempo. El análisis y aplicación del modelo se lo realiza poco a poco, empezando con un modelo sencillo que posteriormente al considerar más variables involucradas se llegue al modelo óptimo que represente fielmente lo que sucede en el deshidratador, una vez obtenido dicho modelo, se hace una verificación del mismo aplicando datos experimentales para realizar una comparación y análisis del modelo.\n"}
{"prompt":"Caracterización acústica de los auditorios de la UDLA, mediante modelos predictivos y mediciones in situ ->","completion":" The acoustic characterization of the enclosures of massive concentration has turned into a priority in order to determine the behavior of the sound to give auditory quality, In case of this work of thesis there was realized an acoustic analysis the University of the Americas´ auditoriums, with measurements in-situ and with simulations predictive with the software E.A.S.E. Electro Acoustics Simulation for Engineers, comparing the results obtained in each of them in the following parameters: C50, C80, vivacity RTmid, warmth BR, brightness Br, STI, STIPA, In order to determine a good characterization and the different types of analysis. After having obtained the results of the different evaluated parameters, so much of it forms in situ like across simulation, is achieved to conclude that the values obtained of the time of reverberation differ between if, verified that there could not decide with exacted the coefficient of absorption and the form of installation of material in every audience.La caracterización acústica de los recintos de concentración masiva se ha convertido en una prioridad con el fin de determinar el comportamiento del sonido para dar calidad auditiva, en el caso de este trabajo de titulación se realizó un análisis acústico de los Auditorios de la Universidad de las Américas, con mediciones in situ y con simulaciones predictivas con el software E.A.S.E. Electro Acoustics Simulation for Engineers, comparando los resultados obtenidos en cada uno de ellos en los siguientes parámetros: C50, C80, Vivacidad TRmid, Calor BR, Brillo Br, STI, STIPA, con el fin de determinar una buena caracterización acústica y los distintos tipos de análisis. Después de haber obtenido los resultados de los diferentes parámetros evaluados, tanto de forma in situ como a través de simulación, se logra concluir que los valores obtenidos del tiempo de reverberación difieren entre sí, comprobado que no se pudo determinar con exactitud el coeficiente de absorción y la forma de instalación del material en cada auditorio.\n"}
{"prompt":"Rediseño de gestión de crédito y cobranza de importadora de repuestos basado en modelos predictivos. ->","completion":" El objetivo principal fue rediseñar la gestión de crédito y cobranza con el fin de disminuir en un 50% los costos anuales asociados a estas actividades. Para el rediseño de la gestión de crédito, se creó modelos predictivos para los días de crédito con regresiones lineales múltiples. El rediseño de la gestión de crédito consiste en la elaboración de una herramienta práctica realizada en Excel, con el fin de estimar los días en los que pagará el cliente. De acuerdo a esta estimación, se indica el descuento apropiado. El rediseño de la gestión de cobranza consiste en la creación de un procedimiento estandarizado, en el que se eliminó o se redujo las actividades que no agregan valor. Se cambió los formatos de los reportes de Cuentas vencidas y de Gestión de cobro para tener mayor agilidad y confiabilidad de la información. Finalmente, se analizó la contratación de un servicio de cobranza externo para cuentas críticas.GuayaquilIngenieras Industriales.\n"}
{"prompt":"Detección del Orden de Lectura de un documento en base a Inteligencia Computacional ->","completion":" En la actualidad, los documentos digitales se han vuelto una parte esencial de nuestra vida cotidiana. Actualmente podemos encontrar un documento digital para casi cualquier libro o documento que necesitemos, pero una gran problemática es que muchos de estos documentos digitales son imágenes guardadas en formato PDF, lo que hace muy difícil la extracción de la información de manera digital. Debido a estas y otras problemáticas se han generado sistemas de procesamiento de imágenes que busca recuperar la información almacenada mediante el Reconocimiento Óptico de Caracteres (OCR) pero una gran limitante de este tipo de sistemas es que no puede definir un Orden de Lectura lógico. El Orden de Lectura no es más que la secuencia lógica de interpretación de la información contenida en un documento. Mediante el procesamiento de documentos en formato PDF y procesamiento digital de imágenes, en este proyecto se busca desarrollar un algoritmo capaz de identificar el Orden de Lectura de un documento que permita extraer su información de forma ordenada. Esto se lo realizará en base a Lógica Difusa, la cual se basa el Razonamiento Aproximado y en el uso de Reglas Lingüísticas. Este proyecto puede ser usado para la recuperación de información y así crear bibliotecas virtuales o aplicaciones que sirvan de ayuda a personas con discapacidad visual.\n"}
{"prompt":"Optimización de parámetros mixtos de arreglos de antenas, utilizando técnicas de computación inteligente ->","completion":" Del desempeño de las antenas depende el nivel de potencia recibido en el equipo receptor, el nivel de la potencia de interferencia en el receptor, etc., en sistemas inalámbricos de telecomunicaciones. En ese particular, es de especial interés el diseño y fabricación de antenas, que contemplen en el proceso de diseño de las mismas, parámetros que estén en concordancia con las especificaciones requeridas según el uso destinado para la antena en estudio.GuayaquilMagíster en Telecomunicaciones\n"}
{"prompt":"Herramienta de reconocimiento facial con técnica de visión computacional 2D ->","completion":" In this thesis an image recognition system has been designed by artificial vision, centered on people's faces. Here, we have studied the complexity of two algorithms that have demonstrated their efficiency and robustness in face recognition applications using computer vision. In addition, a system has been implemented for such purpose based on said algorithms and the proposal of improvements in them. The algorithms mentioned above are: 1) The method Autorostros (Eigenfaces) and 2) The method of Linear Discriminant Analysis (Fisherfaces). Both methods are able to detect faces by comparison with images stored in databases, and among the fundamental characteristics of these methods we can highlight the following: the method of Autorostros is responsible for comparing with a single image of the person who serves as a reference and that is stored in the database; while the Linear Discriminant Analysis method performs the comparison against an average of reference images stored in the database. In this thesis it has been possible to demonstrate experimentally that the first method needs less processing time, but is less robust; while the second method requires a longer processing time, but is more robust. Therefore, the latter is appropriate for working in environments affected by pollution and unwanted signals. With the above background, in order to improve the robustness of the method Autorostros, it is proposed to create a database with a set of reference photos of each of the faces to identify, find a measure of the central tendency of these photos to each face and take this measure as the new reference. However, even doing this, the results are still inferior to the Linear Discriminant Analysis method. Therefore, if a rapid identification system is desired and a certain margin of error is tolerated, the recommended method is the Autorostros. On the other hand, if you want to implement a robust processing system, the appropriate method is Linear Discriminant Analysis.En esta tesis se ha diseñado un sistema de reconocimiento de imágenes por visión artificial, centrado en rostros de personas. Aquí, se ha estudiado la complejidad de dos algoritmos que han demostrado su eficiencia y robustez en aplicaciones de reconocimiento de rostros usando visión por computador. Además, se ha implementado un sistema con tal propósito basado en dichos algoritmos y la propuesta de mejoras en los mismos. Los algoritmos mencionados anteriormente son: 1) El método Autorostros (Eigenfaces) y 2) El método Análisis Discriminante Lineal (Fisherfaces). Ambos métodos son capaces de detectar rostros por comparación con imágenes guardadas en bases de datos, y entre las características fundamentales de dichos métodos podemos destacar lo siguiente: el método de Autorostros se encarga de comparar con una única imagen de la persona que sirve de referencia y que se encuentra guardados en la base de datos; mientras que el método Análisis Discriminante Lineal realiza la comparación contra un promedio de imágenes de referencia guardadas en la base de datos. En esta tesis se ha podido demostrar experimentalmente que el primer método necesita menos tiempo de procesamiento, pero es menos robusto; mientras que el segundo método requiere un tiempo de procesamiento más alto, pero es más robusto. Por lo que, este último es el apropiado para trabajar en entornos afectados por la contaminación y señales no deseadas. Con los antecedentes expuestos, con el objeto de mejorar la robustez del método Autorostros, se propone crear una base de datos con un conjunto de fotos de referencia de cada uno de los rostros a identificar, buscar una medida de la tendencia central de dichas fotos para cada rostro y tomar esta medida como la nueva referencia. Sin embargo, incluso haciendo esto, los resultados siguen siendo inferiores al método Análisis Discriminante Lineal. Por lo tanto, si se desea un sistema de identificación rápido y se tolera un determinado margen de error, el método recomendado es el Autorostros. Por otra parte, si se desea implementar un sistema de procesamiento robusto, el método apropiado es el Análisis Discriminante Lineal.\n"}
{"prompt":"Del pensamiento complejo al pensamiento computacional: retos para la educación contemporánea ->","completion":" Una de las problemáticas de la educación hoy en día es que se continúa privilegiando la enseñanza del contenido sobre el desarrollo de destrezas y habilidades cognitivas que permitan un desarrollo del pensamiento de los estudiantes. Frente a una educación tradicional, se considera que una reflexión del pensamiento complejo puede contribuir a una mejor comprensión de una realidad contemporánea. La educación hoy tiene como reto el explicar y el comprender, desde una perspectiva hermenéutica, las nuevas complejidades de la realidad con la aparición y utilización cotidiana del internet, de las TIC, de la web 2.0 y de las redes sociales. Por este motivo, se plantea una relación entre pensamiento complejo y el pensamiento computacional que incide en un mejoramiento de la calidad educativa. En el presente artículo se iniciará con una reflexión en torno a la educación a partir de la concepción de incertidumbre del pensamiento complejo. Luego se plantearán elementos de conexión entre un pensamiento complejo y un pensamiento computacional a partir del conectivismo y los desafíos de una sociedad 3.0 en la que las tecnologías de la información y comunicación se encuentran incorporadas en la vida cotidiana de los seres humanos. A su vez, una definición de pensamiento computacional nos situará sobre esta nueva forma de pensar a partir de problemas reales a través de una nueva lógica computacional para lograr resoluciones. El pensamiento computacional desafía a la educación contemporánea a incorporar este nuevo enfoque para la solución de problemas, construcción de sistemas y comprensión de la relación prospectiva entre la ciencia, la tecnología y una sociedad 3.0. \/\/ One of the problems of education today is that it continues to favoring the teaching of contents instead applying skills and cognitive abilities that allow a development of the thought of the students. Besides a traditional education, it is considered that a reflection of the complex thought can contribute to a better understanding of a contemporary reality. Education today has as a challenge the explanation and understanding, from a hermeneutic perspective, of the new complexities of reality to the occurrence and the everyday use of the internet, ICTS web 2.0 and social networks. For this reason, the proposal of this article is that there is a relationship between complex thinking and computational thinking that affects an improvement of the quality of education. This research focuses on a reflection on education from the conception of uncertainty and complexity. It refer to the relationship between a complex thought and a computational thinking from connectivism and the challenges of a society 3.0 in which ICT is incorporated into the daily life of human beings. In the other hand, a definition of computational thinking will put us on this new way of thinking from real problems through a new computational logic to achieve solutions. The computational thinking challenges the contemporary education to incorporate this new approach to the solution of problems, building systems and prospective understanding of the relationship between science, technology and society 3.0.\n"}
{"prompt":"Iinfluencia del uso de las tecnologías de la información y comunicación (tic´s) en el rendimiento académico de los estudiantes de educación general básica en la asignatura de computación de la unidad educativa “Ciudad de Montalvo” del Cantón Montalvo_ Provincia de los Ríos en el periodo lectivo 2010-2011. ->","completion":" La velocidad y complejidad con las que se produce el conocimiento deben estar orientadas a satisfacer las necesidades del hombre y su mundo. En la actualidad no encontramos ante una sociedad de cambios, ante una explosión de tecnologías de la información y comunicación, ante una sociedad de la información o “red”. Esta condición \"relacional\" constituye el insumo de una nueva economía denominada \"economía de redes\" o \"economía de la información\". Esta nueva economía permite el intercambio de bienes inmateriales; que, en el campo didáctico-pedagógico, se expresa en los procesos de enseñanza y aprendizaje logrando una interacción entre docentes y estudiantes con una retroalimentación inmediata (chats) sin tener en cuenta el limitante del espacio geográfico o esquivando el temporal (correo-plataformas virtuales). La utilización de las tecnologías de la información y comunicación en el contexto educativo en general y en la educación superior en particular, desempeñan un papel preponderante con la educación superior al servicio de la formación y actualización de los intelectuales y profesionales. Permite una mayor interacción entre docentes y estudiantes, un aprendizaje colaborativo entre estudiantes, un auto-aprendizaje y, fundamentalmente un cambio de actitud en los roles que deben cumplir los docentes y estudiantes.\n"}
{"prompt":"Localización de fallas en sistemas eléctricos de distribución con generación distribuida usando máquinas de soporte vectorial ->","completion":" This paper proposes the Support Vector Machines (SVM) use as tool that allows us to locate the type of fault that can occur in the electrical distribution system, with the presence of Distributed Generation (GD). This approach is based on the measurement of the RMS voltages in both the substation and at the points where the distributed generators are onnected and will be used as descriptors for the SVM, to illustrate the proposed methodology was considered the IEEE 34-bar circuit in which two generators are connected in which the energy penetration will be varied through the short-circuit impedance all types of faults were simulated and the database was constructed using the ATPDraw software. The results show a range of accuracy from 70% to 100% in locating faults, showing that distributed generators increase the accuracy of the vector support machines.En el presente documento se propone el uso de las Máquinas de Soporte Vectorial (SVM) como una herramienta que permita localizar el tipo de falla que se puede presentar en el sistema de distribución eléctrica, con la presencia de Generación Distribuida (GD). Este enfoque se basa en la medición de los voltajes RMS tanto en la subestación como en los puntos donde se conectan los generadores distribuidos y serán utilizados como descriptores para las SVM, para ilustrar la metodología propuesta se consideró el circuito de 34 barras de la IEEE en el cual se conectan dos generadores en el que se ira variando la penetración de energía a través de la impedancia de corto circuito, se simulan todos los tipos de fallas y la base de datos se construye mediante el software ATPDraw. Los resultados muestran un rango de precisión del 70% al 100% en localizar las fallas demostrando que los generadores distribuidos aumentan la precisión de las máquinas de soporte vectorial.\n"}
{"prompt":"Estudio de la autonomía del vehículo eléctrico Kia Soul aplicando máquinas de soporte vectorial en la ciudad de Cuenca ->","completion":" Este Proyecto estudia la autonomía de un vehículo 100% eléctrico, que mediante la aplicación de Máquinas de Soporte Vectorial para regresión, se encuentra un modelo matemático de la autonomía dependiendo de algunas variables tomadas con el vehículo en circulación, con las SVM mejoramos otros métodos de estudio, obteniendo respuestas más precisas.This Project studies the autonomy of a 100% electric vehicle, which through the application of vector support machines for regression, is a mathematical model of autonomy depending on some variables taken with the vehicle in circulation, with the SVM we improve other methods of study, obtaining more precise answers.\n"}
{"prompt":"Desarrollo de un sistema seguidor de tendencia en base a máquina de soporte vectorial optimizado por cúmulo de partículas para pares de divisas ->","completion":" El presente proyecto de investigación desarrolló un sistema para predecir la continuidad y cambio de tendencia de pares de divisas del mercado FOREX. La motivación surge de la necesidad de poner al alcance de los inversores modelos predictivos fundamentados en Inteligencia Artificial que permitan tomar mejores decisiones y generar rentabilidad. El sistema fue elaborado con el software de programación Python en sincronismo con la plataforma Metatrader 5, y adicionalmente se diseñó una interfaz gráfica, para que en conjunto agiliten el análisis de los pares disminuyendo los tiempos para la generación de gráficos, evitando la engorrosa tarea de tener que descargar datos manualmente. El funcionamiento predictorio se efectúa mediante el cruce de dos hiperplanos, los cuales fueron optimizados por algoritmo de enjambre de partículas (PSO). El estudio de los cruces históricos de los hiperplanos permite controlar el riesgo que se asume al efectuar una operación comercial debido a que filtra el ruido en los datos. En caso de querer realizar una operación de compra se considera al menos los dos últimos cruces con proyección alcista y se define el Stop Loss, a partir de promediar la cantidad de pips que movió el par antes de cada cruce. La estrategia que se elaboró a partir de este sistema se evaluó en tres pares de divisas diferentes (EURUSD, USDJPY, NZDUSD) en temporalidades de 1hora, 15 y 5 minutos. De los resultados obtenidos se puede destacar que el indicador de cruce de hiperplanos y la variante de MACD demostró tener mayor acertividad y rentabilidad en temporalidades de 5 y 15 minutos en comparación con la temporalidad de una hora, siendo una estrategia útil para operar con una estrategia de Scalping, la cual se basa en realizar un gran número de operaciones a corto plazo. El documento consta de cuatro capítulos distribuidos de la siguiente manera: introducción, marco referencial, metodología y diseño, resultados y conclusiones.This research project developed a system to predict the continuity and change of trend of currency pairs in the FOREX market. The motivation arises from the need to make predictive models based on Artificial Intelligence available to investors that allow them to make better decisions and generate profitability. The system was developed with Python programming software in synchronism with the Metatrader 5 platform, and in addition a graphical interface was designed, so that together they speed up the analysis of pairs, reducing the times for the generation of graphs, avoiding the cumbersome task of having to download data manually. The predictive operation is carried out by crossing two hyperplanes, which were optimized by the particle swarm algorithm (PSO). The study of the historical crossovers of the hyperplanes allows to control the risk that is assumed when carrying out a commercial operation because it filters the noise in the data. In case of wanting to carry out a buy operation, at least the last two crosses with an upward projection are considered and the Stop Loss is defined, based on averaging the number of pips that the pair moved before each crossing. The strategy that was developed from this system was evaluated in three different currency pairs (EURUSD, USDJPY, NZDUSD) in time frames of 1 hour, 15 and 5 minutes. From the results obtained, it can be noted that the hyperplane crossover indicator and the MACD variant proved to have greater accuracy and profitability in timeframes of 5 and 15 minutes compared to the timeframe of one hour, being a useful strategy to operate with a strategy Scalping, which is based on carrying out a large number of short-term operations. The document consists of four chapters distributed as follows: introduction, frame of reference, methodology and design, results and conclusions.\n"}
{"prompt":"Reconocimiento de imágenes mediante redes neuronales ART ->","completion":" Consideramos de gran utilidad investigar acerca de los principios de ART1 dado que es un algorítmo que no goza de la misma difusión de otros como perceptrón, perceptrón multicapa o mapas autoorganizativos de Kohonen. Otra razón importante es el sinnúmero de aplicaciones que posee ART1 que incluyen a parte del reconocimiento automático de blancos, previsiones financieras, monitoreo de máquinas, diseño de circuitos digitales, análisis químico y visión de robots.\n"}
{"prompt":"Estudio de redes neuronales y aplicaciones prácticas ->","completion":" En su descripción monográfica escrita los autores opinan que las Redes de Neuronas Artificiales son un tipo de aprendizaje y de procedimientos automático insi prtadoa en el funcionamiento del sistema nervioso. Estas simulan las propiedades de los sistemas neuronales biológicos a través de modelos matemáticos recreados mediante mecanismos artificiales, tratando de modelar esquemáticamente la estructura hardware del cerebro. Estos sistemas de procesamiento de informacióm, paralelos, distribuídos y adaptivos, son capaces de aparender de la experiencia a partir de datos del entorno y empleando algoritmos numéricos\n"}
{"prompt":"Clasificación de los sonidos cardíacos usando ondículas y redes neuronales ->","completion":" The auscultation of cardiac sounds is a clinical examination that allows to determine if a patient should be referred to a specialist. The phonocardiogram (PCG) corresponds to the recording of these sounds. The objective of this work is the evaluation of the combination of two of the proposed algorithms during PhysioNet 2016 challenge, the first is based on wavelets and the second on a neural convolutional network to evaluate the performance in the classification of cardiac sounds (normal\/abnormal). The results show a better balance between specificity and sensitivity with respect to the wavelet method, although its performance is inferior to the method based on neural networks. The proposed method has a lower computational cost.La auscultación de los sonidos cardíacos es un examen clínico que permite determinar si un paciente debe ser referido a un especialista. El fonocardiograma (PCG), por sus siglas en inglés, corresponde al registro de estos sonidos. El objetivo de este trabajo es la evaluación de un esquema fundamentado en dos algoritmos propuestos durante el desafío PhysioNet 2016, el primero basado en ondículas y el segundo en una red neuronal convolucional (RNC), para evaluar el desempeño en la clasificación de los sonidos cardíacos (normal\/anormal). Los resultados obtenidos muestran un mejor equilibrio entre especificidad y sensibilidad con respecto al método de las ondículas, aunque su desempeño es inferior al método basado en RNC. El método propuesto tiene un menor costo computacional.\n"}
{"prompt":"Locomoción de un robot cuadrúpedo basada en redes neuronales artificiales ->","completion":" Se diseñó y construyó un robot cuadrúpedo con locomoción basada en la red neuronal artificial perceptrón multicapa, para conocer si logra mantener la estabilidad durante la caminata sobre terrenos irregulares. Se utilizó el método inductivo para el desarrollo del diseño estructural, la circuitería interna, el algoritmo de control y la interfaz de monitoreo. La aplicación del método experimental permitió verificar el funcionamiento del robot. Las piezas que componen el cuerpo y las patas del robot fueron construidas mediante la técnica de impresión 3D con plástico PLA. El algoritmo de control para la locomoción del robot y la interfaz de monitoreo de datos de: sensores ultrasónicos, acelerómetro y velocidad, temperatura y voltaje de cada uno de los motores fueron desarrollados en MATLAB R2010a. La comunicación se realiza inalámbricamente entre la placa microcontroladora Arduino del robot y el ordenador mediante dispositivos Xbee y como fuente de alimentación se utilizaron dos baterías Li-Po. Mediante el análisis de los datos se comprobó que el robot cuadrúpedo no muestra una variación significativa de inclinaciones comparadas con las ideales, por lo tanto mantiene la estabilidad a pesar de irregularidades y obstáculos presentes en el terreno sobre el cual se desplaza. El robot cuadrúpedo podría utilizarse en procesos industriales de transporte de materiales o sustancias peligrosas y exploración de ambientes con terrenos irregulares, de difícil acceso o contaminados.A quadruped robot with locomotion was designed and build based on the artificial neural network perceptron multilayer, to find out if it can keep the stability while walking on uneven terrain. The inductive method was used for the structural design, the internal circuitry, the control algorithm and interface monitoring. The experimental method application allowed verifying the operation of the robot. The component parts of the body and legs of the robot were constructed by 3D printing technique with PLA (Poly Lactic Acid) plastic. The control algorithm for the robot locomotion and monitoring interface data of: ultrasonic sensors, accelerometer and speed, temperature and voltage of each one of the servomotors were developed in MATLAB R2010a. Communication in done wirelessly between the Arduino motherboard plaque and the computer through Xbee devices and as a main power supply two batteries Li-Po were used. By analyzing the data it was found that the quadruped robot shows no significant variation compared with the ideal inclinations, therefore maintains stability despite irregularities and obstacles in the terrain over which it travels. The quadruped robot could be used in industrial processes transporting hazardous materials or substances and exploration of irregular terrain, inaccessible or contaminated.\n"}
{"prompt":"Redes Neuronales Artificiales para Predecir el Riesgo de Insolvencia ->","completion":" En las organizaciones la predicci?n del riesgo de insolvencia llega a ser una arista muy importante en el ?rea financiera dado que se requiere informaci?n anticipada para poder enfrentar a tiempo los problemas econ?micos que puedan presentarse. Tener una buena gesti?n del riesgo financiero prepara con antelaci?n las decisiones y estrategias a tomarse para perdurar en el tiempo. El presente art?culo presenta los conceptos claves del riesgo de insolvencia, as? como su importancia y la teor?a aplicada en los diferentes modelos que se han usado para la predicci?n del riesgo de insolvencia. El modelo que destac? es el uso de la red neuronal artificial, se analiz? las diferentes aplicaciones con redes neuronales artificiales que se han desarrollado para el c?lculo del riesgo de insolvencia de manera predictiva, observando las estructuras b?sicas de la red en cada aplicaci?n, analizando las ventajas y desventajas que se pueden presentar al utilizar este modelo.Mag?ster en Administraci?n de Empresas, menci?n Finanzas\n"}
{"prompt":"Reconocimiento de firmas usando redes neuronales artificiales (RNA) ->","completion":" Antecedentes. Reconocimiento de patrones. Redes neuronales. Desarrollo del sistema aplicacional. Implementación y prueba del sistema. Conclusiones y recomendaciones. Anexos.\n"}
{"prompt":"Pérdida de memoria a corto plazo en los estudiantes de la Unidad Educativa Rosa Zárate ->","completion":" This research work is aimed at a bibliographic review with different authors, whichproposes as a general objective; to analyze short-term memory loss in first and second-year high schoolstudents from the Rosa Zárate Educational Unit, through research withscientific articles. Short- term memory is itself a temporary storage and fast programming, in which said collected information is detailed, demonstrated, andestablished for later storage, where the informationis retained for a minimum period ofapproximately twenty seconds. On the other hand, short-term memory works actively,which is why it is called working memory. The research presentsa quantitative approachwith a non-experimental and descriptive design in itself, taking into account what wasanalyzed and what are the characteristics presented by the first and second high schoolstudents of the Rosa Zárate educational Unit. For data collection, first and secondhighschool students were evaluated with a population of 60 students, including men and women, where the test Developed by Gilewski, Zelinsky and Sahie, (1990) validated by Alarcón, Rubio, and Fernández was applied (2008) where it is composed of 31 items.El presente trabajo de investigación está encaminado en una revisión bibliográficacon diferentes autores, que propone como objetivo general; analizar la pérdida dememoria a corto plazo en los estudiantes de primero y segundo de bachillerato de la unidad Educativa Rosa Zárate, mediante una investigación con artículos científicos. La memoria a corto plazo se trata en si de un almacenamiento temporal y de programación rápida, en lo cual dicha información recopilada es detallada, demostrada y establecida para su posterior almacenamiento, donde la información es retenida durante un periodo mínimo de tiempo aproximadamente de veinte segundos. Por otra parte, la memoria a corto plazo trabaja de manera activa, es por eso por lo que se lo denomina memoria operativa de trabajo. La investigación presenta un enfoque cuantitativo con un diseño noexperimental y descriptivo en si tomando en cuenta lo que se analizó y cuáles son las características que presentan los estudiantes de primero y segundo de bachillerato de la unidad educativa Rosa Zárate. Para la recopilación de datos fueron evaluados los estudiantes de primero y segundo de bachillerato con una población de 60 estudiantes entre ellos hombres y mujeres donde fue aplicado el test Desarrollado por Gilewski, Zelinsky y Sahie, (1990) validado por Alarcón, Rubio, y Fernández. (2008) donde estar compuesto por 31 ítems.UNACH, Ecuador\n"}
{"prompt":"La lúdica para fortalecer la memoria a corto plazo en la asignatura de matemática de los estudiantes del colegio Don Bosco de la ciudad de Quito ->","completion":" La memoria a corto plazo juega un papel muy importante en la vida del ser humano, debido a que permite retener y manipular temporalmente información, mientras participa en procesos de aprendizaje, comprensión y razonamiento. Actualmente en el contexto ecuatoriano la matemática es la asignatura que más problemas presencia, sea ya por la complejidad, el poco de interés de los educandos o debido a las estrategias tradicionales que se aplican para su enseñanza, Ecuador participó por primera vez en las pruebas PISA en al año 2018, donde el 70,9% de los estudiantes no alcanzaron el nivel de desempeño básico. En la ciudad de Quito en la escuela particular Don Bosco, en los estudiantes de sexto y séptimo año se observó que solo el 1% de la población investigada alcanza un desarrollo adecuado de la memoria a corto plazo en relación a su rango de edad, 11 y 12 años, presentan dificultad en la retención de información para el aprendizaje de los contenidos matemáticos. La presente investigación tiene como objetivo determinar la influencia de la lúdica como herramienta para el fortalecimiento de la memoria a corto plazo en la asignatura de matemáticas en los estudiantes del colegio Don Bosco. El estudio es de carácter cuasi experimental mediante la aplicación del test Wisc de dígitos como estrategia en la recolección de información. Se concluye que la lúdica influye en el desarrollo de la memoria a corto plazo en la matemática, pues las calificaciones negativas en esta asignatura pasaron del 31% en el tercer parcial al 5% en cuarto parcial, disminuyendo en un 26% la cantidad de estudiantes que no logran nota mínima de 7,00 puntos. Además, la cantidad de estudiantes que alcanzaron el rango de edad adecuado, paso de ser el 1% al 40%, según la aplicación del post test Wisc.\n"}
{"prompt":"Trastornos de la memoria a corto plazo en los adultos mayores del hogar geriátrico Daniel Álvarez Sánchez de la ciudad de Loja ->","completion":" The purpose of this research is to show and to detect the presence of memory disorders short term and potential risk factors that trigger the aforementioned disorders in older adults residing her nursing home Jan Daniel Alvarez Sanchez de Loja, as data from research conducted in populations of 50 and 95 years show high prevalence of impaired short-term memory, being more frequent memory disorders in women and in people with risk factors such as stress, habit smoking, alcohol consumption, history of head trauma, drug use, exposure to trauma and low education. This research was descriptive, quantitative and transversal, with the data collection method using a data-data collection from medical records of the inmates, plus an interview about the risk factors that trigger memory disorders used short-term and finally a test called CODEX, for the detection of disorders of short-term memory in older adults was applied. Results from this investigation showed the existence of memory disorders short term 30% of the population studied, showing a low incidence of impaired short term memory; between the main memory disorders in the short term that was found in this population were; Alzheimer's by 33%, 27% dementia, amnesia and 13% respectively Korsakof syndrome, delirium and 7% respectively retrograde amnesia. Among the risk factors, we have 58% alcohol, snuff consumption 54%, 14% drug use, exposure to traumatic events 26%, exposure to stressful situations 52%. What we can conclude that the disorder short term memory in older adults the nursing home Daniel Alvarez Sanchez de Loja occur in low incidence and that the main risk factors found were alcohol, snuff and drugs ; low educational level, exposure to traumatic and stressful situations. Keywords: memory disorders, aging, risk factors, memory.El propósito de este trabajo de investigación es mostrar y detectar la presencia de TRASTORNOS DE MEMORIA A CORTO PLAZO y los posibles factores de riesgo que desencadenan los mencionados trastornos en los adultos mayores que residen en el hogar geriátrico Daniel Álvarez Sánchez de la ciudad de Loja, ya que datos de investigaciones realizadas a nivel nacional, muestran alta prevalencia de trastornos de memoria a corto plazo, siendo más afectado el sexo femenino, personas estresadas, fumadores, consumistas de alcohol, personas que sufrieron traumatismos craneoencefálicos, por uso de otras drogas, por estar expuestas a situaciones traumáticas. Esta investigación fue de tipo descriptiva, cuantitativa y transversal, teniendo como método el uso de una ficha de recolección de datos de las historias clínicas de los asilados, además se utilizó una entrevista sobre los factores de riesgo que desencadenan trastornos de memoria a corto plazo y finalmente se aplicó un test llamado CODEX, destinado a la detección de trastornos de memoria a corto plazo. Los resultados arrojados por esta investigación mostraron la existencia de TRASTORNOS DE MEMORIA A CORTO PLAZO en un 30% de la población estudiada, mostrando una baja incidencia de los mencionados trastornos; entre los principales trastornos que se encontró en esta población fueron; alzhéimer en un 33%, demencia senil 27%, amnesia anterógrada y síndrome de korsakof 13%, delirium y amnesia retrograda 7 %, hay que mencionar que el Alzheimer, amnesia anterógrada, síndrome de korsakof y delirium son trastornos de memoria de corto y largo plazo. Entre los factores de riesgo presentes tenemos consumo de alcohol 58%, 54% consumo de tabaco, 14% consumo de otras drogas, exposición a situaciones traumáticas 26%, exposición a situaciones estresantes 52%. Con lo que podemos concluir que los trastornos de memoria a corto plazo en los adultos mayores del hogar geriátrico Daniel Álvarez Sánchez de la ciudad de Loja se presentan en baja incidencia y que los principales factores de riesgo encontrados fueron el consumo de alcohol, tabaco y drogas; bajo nivel educativo, exposición a situaciones traumáticas y estresantes. Palabras clave: trastornos de la memoria, envejecimiento, factores de riesgo, memoria.\n"}
{"prompt":"Comparación entre redes neuronales artificiales y regresión múltiple para la predicción de la rugosidad superficial en el torneado en seco ->","completion":" Los métodos de regresión múltiple y redes neuronales artificiales son técnicas usadas en muchas aplicaciones de la industria. En este trabajo se utilizaron dos métodos de predicción: regresión múltiple y redes neuronales artificiales (perceptrón multicapa) con el objetivo de predecir la rugosidad superficial en el torneado en seco del acero AISI 316l. En su implementación fueron considerados varios parámetros de corte como la velocidad, el avance y el tiempo de mecanizado. Las ecuaciones obtenidas por ambos métodos fueron comparadas desarrollando un diseño factorial completo para aumentar la fiabilidad de los valores registrados de rugosidad superficial. En el análisis se puede comprobar mediante los valores de coeficientes de determinación que los modelos propuestos son capaces de predecir la rugosidad superficial. Los modelos obtenidos demuestran que la técnica de redes neuronales artificiales tiene mejor precisión que la regresión múltiple para este estudio.\/\/ The simple regression and artificial neural network methods are techniques used in many industrial aplications. This work developed two models in order to predict the surface roughness in dry turning of AISI 316L stainless steel. In its implementation they were considered various cutting parameters such as cutting speed, feed, and machining time. The models obtained by both methods were compared to develop a full factorial design to increase reliability of the recorded values of roughness. The analysis can be corroborated by the values of coefficients of determination that the proposed models are able to predict for surface roughness. The obtained results show that the neural networks techniques are more accurate than the multiple regression techniques for this study.\n"}
{"prompt":"Diseño de un modelo para el reconocimiento de patrones de escritura mediante uso de redes neuronales artificiales ->","completion":" Esta tesis tiene como línea general el diseñar un modelo para reconocer patrones de escritura, basado en las actualmente y exitosas redes neuronales artificiales, por los grandes avances que ya se han registrado en diferentes estudios realizados. En la primera fase se comienza con un estado en el que se encuentra estas dos importantes áreas, reconocimiento de patrones y redes neuronales, detallando los tipos de estas y los algoritmos de entrenamiento que existen para iniciar con el estudio del más apropiado que nos sirva en el proyecto. La segunda fase se describe el problema para diseñar la solución, aquí también se hace referencia a la herramienta donde se diseñara la implementación para las pruebas. Se detalla en la fase tres como se realiza la implementación, y el programa desarrollado en MatLab, junto con las pruebas del modelo. La fase 4 se realiza la discusión de resultados, para a sus ves dar las conclusiones y recomendaciones necesarias.This thesis has as a general line the design of a model to recognize writing patterns, based on the currently successful artificial neural networks, for the great advances that have already been recorded in different studies. In the first phase we start with a state in which we find these two important areas, pattern recognition and neural networks, detailing the types of these and the training algorithms that exist to start with the most appropriate study that serves us in the project. The second phase describes the problem to design the solution, here also refers to the tool where the implementation for the tests will be designed. It is detailed in phase three as the implementation is done, and the program developed in MatLab, along with the tests of the model. Phase 4 is the discussion of results, so you can give the necessary conclusions and recommendations.\n"}
{"prompt":"Diseño de un modelo para la detección de obstáculos mediante la utilización de las redes neuronales artificiales ->","completion":" El presente trabajo se pretende aprovechar la capacidad de generalización de redes neuronales para resolver los problemas que se plantean en la navegación de un robot móvil en un ambiente desconocido. Hoy en día la ciencia y los avances tecnológicos han logrado que se pueda desarrollar de forma verás y objetiva redes neuronales a través un sin número de herramientas para su diseño, vasta tener conocimientos básico del funcionamiento de las mismas, una de las más utilizadas es Matlab que cuenta con una serie de librerías que se dedican exclusivamente a este fin, además proporciona un entorno amigable para el programador. Esta tesis esta dividida en 4 fases: En la primera se presenta el objetivo de la investigación que abarca el estudio de las características de las redes neuronales, reconocimiento de patrones y robots; en la segunda se describe el diseño a desarrollarse en el ambien Ete virtual SimRobot; en la tercera fase se aplican los algoritmos desarrollados para el modelo junto a las pruebas experimentales que se describen en esta fase, en la cuarta fase se trata acerca de las discusiones del proyecto de investigación y en la última se exponen las conclusiones y recomendaciones a futuro.The present work intends to take advantage of the generalization capacity of neural networks to solve the problems that arise in the navigation of a mobile robot in an unknown environment. Today, science and technological advances have been able to develop vertically and objectively neural networks through a number of tools for their design, vast have basic knowledge of the operation of them, one of the most used is Matlab Which has a series of bookstores dedicated exclusively to this purpose, in addition provides a friendly environment for the programmer. This thesis is divided in 4 phases: In the first one the objective of the investigation is presented that includes the study of the characteristics of the neural networks, recognition of patterns and robots; The second describes the design to be developed in the virtual environment SimRobot; In the third phase the algorithms developed for the model are applied together with the experimental tests described in this phase, the fourth phase is about the discussions of the research project and in the last one the conclusions and recommendations are presented in the future .\n"}
{"prompt":"Estrategia metodológica a través del pensamiento computacional para el aprendizaje de Matemática ->","completion":" Los resultados de las pruebas SER, evidencia bajo nivel de conocimientos en matemática, esto por la vigencia del modelo caduco de enseñanza. La investigación se orienta a desarrollar una estrategia metodológica con el pensamiento computacional para el aprendizaje de matemática en los estudiantes de básica superior de la Unidad Educativa “Francisco Flor”. Para lo cual se fundamenta sobre el Pensamiento Computacional con respaldo en autores actuales, la metodología aplicada parte de recopilar información documental y bibliográfica mediante el Perish versión 7, se aplica métodos teóricos y empíricos, el diagnóstico usa la técnica de la observación directa, encuestas a docentes y estudiantes. La problemática, el desconocimiento de los docentes el uso de estrategias del pensamiento computacional para el aprendizaje; la confiabilidad de los instrumentos se realizó a través de Alpha de CronBach quien dio como resultado alto 0,898, esto permite el conocimiento significativo de las principales características del tema de estudio. Propone una estrategia lúdica basada en los juegos dentro de diversos contextos del juego, se estructura las fases del proceso, primera fase, el diagnóstico que determina el contexto actual, segundo fase, la planificación del diseño de la estrategia metodológica con el uso de Google Blocky, la fase tres, la aplicación y familiarización con los bloques de rompecabezas, laberintos con sus niveles y la cuarta fase, el control y verificación del cumplimiento de las actividades de aprendizaje mediante la aplicación del retest a los estudiantes observados valida de esta manera la estrategia para la incorporación del PC en el aprendizaje de Matemáticas.Pontificia Universidad Católica del Ecuador, Dirección de Investigación y PosgradosMagíster en Tecnologías para la Gestión y Práctica Docente\n"}
{"prompt":"Desarrollo de una aplicación multimedia para la enseñanza de relaciones y funciones de matemática superior ->","completion":" Introducción. Objetivos y alcance del proyecto. Marco teórico. Desarrollo de la aplicación.\n"}
{"prompt":"Construcción de espacios asociados en análisis de Clifford usando una implementación computacional basada en matrices ->","completion":" Two di erential operators F and G are associated if F maps solutions of the di erential equation Gu = 0 into solutions of the same equation, that is, F is associated to G if G(Fu) = 0. A function space is a vector space whose objects are functions. The function space containing the solutions of Gu = 0 is called the space associated with the operator F...Dos operadores diferenciales F y G est an asociados si F mapea soluciones de la ecuaci on diferencial Gu = 0 en soluciones de la misma ecuaci on, es decir, F est a asociado a G si G(Fu) = 0. Un espacio funcional es un espacio vectorial cuyos objetos son funciones. El espacio funcional que contiene las soluciones de Gu = 0 se llama espacio asociado al operador F...\n"}
{"prompt":"El conectivismo pedagógico en la transferencia de conocimientos ->","completion":" PDFEl trabajo investigativo se basa en la necesidad constante de una transformación profunda en la enseñanza, para realzar la calidad del aprendizaje de los escolares en la actualidad, dicha transición propone retos e innovación, ante un mundo cambiante, para los alumnos de séptimo grado de educación general básica, de la Unidad Educativa “Dr. Alfredo Raúl Vera Vera” en la ciudad de Guayaquil, el objetivo propone realizar una guía interactiva que influya en la transferencia de conocimientos, se utiliza un enfoque mixto de investigación bibliográfica y de campo, se aplica encuesta de cinco preguntas a las autoridades y entrevista de diez ítems a docentes y estudiantes, basada en la escala de Likert, las cuales son plasmadas en los cuadros y gráficos estadísticos. Se concluye manifestado que el conectivismo pedagógico favorece la transferencia de conocimientos, ya que los educandos pueden reforzar los aprendizajes y reducir las debilidades en el proceso enseñanza aprendizaje.The investigative work is based on the constant need for a profound transformation in teaching, to enhance the quality of learning of schoolchildren today, this transition proposes challenges and innovation, in the face of a changing world, for students in seventh grade of education basic general, of the Educational Unit “Dr. Alfredo Raúl Vera Vera ”in the city of Guayaquil, the objective proposes to make an interactive guide that influences the transfer of knowledge, a mixed approach of bibliographic and field research is used, a survey of five questions is applied to the authorities and an interview of ten items to teachers and students, based on the Likert scale, which are reflected in the tables and statistical graphs. It is concluded that pedagogical connectivism favors the transfer of knowledge, since students can reinforce learning and reduce weaknesses in the teaching-learning process.\n"}
{"prompt":"Clústers : ¿generadores de transferencia de conocimiento, tecnología e innovación en el Ecuador ->","completion":" Los retos de la globalización y los grandes cambios productivos han forzado a los diferentes sectores a considerar estrategias enfocadas en el desarrollo del conocimiento, investigación, avance tecnológico e innovación dentro concentraciones geográficas de empresas (clúster) que influyen directamente en los cambios de los proceso productivos. Para ello una sociedad debe contar con recursos y condiciones que les permitan aprovechar las oportunidades existentes y generar ideas nuevas en base a la calidad de la formación académica, participación activa de los agentes públicos y privados, y la articulación del conocimiento con la tecnología e innovación; teniendo como resultado la productividad, competitividad y crecimiento económico.\n"}
{"prompt":"Aseguramiento de la transferencia de conocimiento en puestos claves en una empresa comercializadora ->","completion":" 1. Problema de investigación. --2. Marco Teórico. --3. Metodología del trabajo. --4. Análisis e interpretación de resultados. --5. Propuesta. --6. Conclusiones y Recomendaciones.El interés en el presente proyecto de investigación, está enfocado en la importancia sobre la transferencia y gestión del conocimiento. El desarrollo de este trabajo parte de una revisión bibliográfica y de la construcción de un marco conceptual en el que se determinó los referentes teóricos y metodológicos necesarios para el diseño de un proceso de transferencia del conocimiento empresarial. Posteriormente se describen los puestos claves en base a la información que facilitó la empresa; se revisó esta información y se detectó la necesidad de modificación de la misma con nuevos criterios. Se trabajó con las personas designadas en los puestos claves y sucesores, para los puestos claves se creó y aplicó a un representante por cargo el instrumento caracterización de puestos. Al obtener los puestos claves se procedió a crear un plan de sucesión considerando a dos candidatos por cada puesto clave a los cuales se les aplicó el segundo instrumento llamado adecuación persona puesto. También se analizó los manuales de funciones para poder proceder a la creación del modelo del sistema, el mismo que fue validado por el criterio de expertos y por la empresa beneficiada. Así como también sustentada por una entrevista aplicada a la gerente de talento humano de la empresa. El principal objetivo de este proyecto fue diseñar una estrategia que asegure la transferencia del conocimiento dentro de la empresa, por medio de instrumentos que permiten el uso y la construcción del conocimiento.Disertación (Psicóloga Organizacional) - Pontificia Universidad Católica del Ecuador, Escuela de Psicología.Psicóloga Organizacional\n"}
{"prompt":"Compresión de datos con Algoritmo de Huffman para transmisión de datos mediante Li-Fi utilizando hardware libre. ->","completion":" In recent years, communication through visible light has become one of the most important developments in telecommunications, as it will allow significant improvements in terms of the speed at which information is transmitted and its security. Considering this, the present project focuses on the construction of a unidirectional Li-Fi communication prototype which, together with the implementation of the Huffman coding algorithm, allows to reduce the total size of the data to be transmitted to be reduced and also the use of the spectrum within the communication link, thus increasing the alternatives that allow optimizing the use of the communication channel. It is also presented a comparative analysis between Arduino and Raspberry Pi as well as the software available for each one, which allow us to obtain the best results and above all, that are economically accessible. Finally, the different tests performed regarding speed parameters and communication spectrum utilization are shown, as well as their respective analysis within two scenarios, the first one without implementing the Huffman coding algorithm and the second scenario making use of said coding ; for such tests the use of the Agilent oscilloscope has been made.La comunicación mediante la luz visible se ha convertido en los últimos años en uno de los avances más importantes dentro de las telecomunicaciones, pues permitirá alcanzar mejoras significativas con respecto a la velocidad en la que se transmite la información y su seguridad. Teniendo en cuenta esto, el presente proyecto se centra en la construcción de un prototipo de comunicación Li-Fi unidireccional que junto con la implementación del algoritmo de codificación de Huffman, permita disminuir el tamaño total de los datos a transmitir y la utilización del espectro dentro del enlace de comunicación, aumentando así, las alternativas que permitan optimizar el uso del canal de comunicación. Se presenta también un análisis comparativo entre Arduino y Raspberry Pi así como el software disponible para cada uno de estos, los cuales permitan obtener los mejores resultados y sobre todo, que sean accesibles económicamente. Finalmente, se muestran las distintas pruebas realizadas con respecto a parámetros de velocidad y utilización del espectro de comunicación, así como sus respectivos análisis dentro de dos escenarios, el primero sin implementar la codificación Huffman y, el segundo escenario haciendo uso de dicha codificación; para tales pruebas se ha hecho uso del osciloscopio Agilent.\n"}
{"prompt":"Modelo de compresión de imágenes de extensión JPEG mediante Matlab para el mejoramiento de la transmisión de datos ->","completion":" El presente trabajo investigativo que se detalla a continuación, corresponde al estudio sobre la compresión de imágenes de extensión .jpeg y como este proceso reduce los recursos esenciales para su almacenaje y mejoramiento de su transmisión. Este informe cubre ciertos aspectos del análisis compresión de datos, profundizando acerca la Transformada Discreta Wavelet (DWT) y cómo ésta se puede utilizar para la compresión de imágenes de extensión .jpeg. Su utiliza la herramienta Guide de Matlab para la elaboración de una aplicación que cumple el proceso de compresión – descompresión de las imágenes de extensión .jpeg, a más de incluir en este proyecto un manual de usuario para su fácil manipulación, permitiendo la visualización de las imágenes obtenidas durante el proceso. En la aplicación también se visualizan los datos obtenidos que sirven para el análisis de compresión de las imágenes. Con el uso del software libre llamado HFS (HTTP File Server) versión 2.2f, se crea un servidor gratuito que permite subir y descargar archivos y es muy útil para analizar la transmisión de las imágenes originales con sus respectivas imágenes comprimidas y descomprimidas. Por último, se realizan las conclusiones y recomendaciones luego de finalizar el desarrollo del proyecto y haber realizado pruebas para la comprobación del funcionamiento de la aplicación realizada.\n"}
{"prompt":"Estudio y análisis de la compresión de datos de almacenamiento de información, diseño e implementación del código de Huffman adaptativo. ->","completion":" Este proyecto que se realiza en este trabajo, tiene por objetivo el de implementar un sistema que realice la compresión y descompresión de información. Para esto va a ser muy útil el empleo del paquete MATLAB, el mismo que se define como un Laboratorio Matricial. Con la ayuda del paquete MATLAB, se desarrollaran las funciones que permitan realizar la compresión y descompresión de la información a través de la implementación del Código Huffman Adaptativo, las mismas que constituyen los programas para la aplicación. Para el desarrollo del presente proyecto se elaboraron los siguientes capítulos: En el capítulo I, se realiza un análisis de la teoría de la información y una introducción a la medida de la misma, dando conceptos básicos que forman la base para los procesos aleatorios. En el capítulo II, se presenta un estudio de la generación de la información, analizando las fuentes generadoras de símbolos con sus respectivas probabilidad de ocurrencia y términos empleados por las mismos.\n"}
{"prompt":"Sistema de predicción de la complejidad léxica implementando machine learning y redes neuronales para reducir barreras de la compresión lectora en los estudiantes universitarios. ->","completion":" PDFLa identificación de palabras complejas (CWI) es la tarea de detectar en el contenido de los documentos las palabras que son difíciles o complejas de entender por las personas de un determinado grupo. El objetivo de esta investigación es el desarrollo de un sistema de predicción de la complejidad léxica tanto del idioma inglés como del idioma español. El sistema se basa en la implementación de características lingüísticas a nivel de la palabra y oraciones, y en la implementación de las técnicas de redes neuronales BERT y XLM-RoBERTa para la generación de nuevas características que permitan resultados mucho más precisos. Se aplicó el algoritmo Random Forest Regressor. Para el entrenamiento de algoritmo se utilizó un conjunto de datos conformado por un corpus de textos en español y otro corpus de textos en inglés. La evaluación del algoritmo se lo realizó mediante la partición 90% - 10%. La metodología de desarrollo que se aplicó fue Kanban, y la metodología de investigación se basó en el Estudio de Caso por lo cual sus unidades de análisis se fundamentaron en las características lingüísticas generadas. Tras varias ejecuciones del algoritmo fue necesario implementar una validación cruzada de 5 variaciones para lograr resultados más precisos. El sistema será de mucho beneficio para la generación de soluciones dirigidas a los estudiantes con bajo nivel de comprensión lectora.Complex Word Identification (CWI) is the task of detecting in the content of documents words that are difficult or complex to understand by the people of a certain group. The objective of this research is the development of a system for predicting the lexical complexity of both the English and Spanish languages. The system is based on the implementation of linguistic characteristics at the level of the word and sentences, and in the implementation of BERT and XLM-RoBERTa neural network techniques for the generation of new features that allow much more accurate results. The Random Forest Regressor algorithm was applied. For the algorithm training, a dataset consisting of a corpus of texts in Spanish and another corpus of texts in English was used. The evaluation of the algorithm was performed by partitioning 90% - 10%. The development methodology that was applied was Kanban, and the research methodology was based on the Case Study, for which its units of analysis were based on the linguistic characteristics generated. After several runs of the algorithm, it was necessary to implement a cross validation of 5 variations to achieve more accurate results. The system will be of great benefit for the generation of solutions aimed at students with a low level of reading comprehension.\n"}
{"prompt":"Procesamiento de datos de espectrometría de masas: algoritmos y metodologías ->","completion":" El cáncer es una enfermedad asintomática en una etapa temprana y muy difícil de diagnosticar. En muchos de los casos no es percibida hasta que ya alcanza la metástasis. Es la segunda causa de muerte en el Ecuador a pesar de que los avances conseguidos en los últimos tiempos han sido revolucionarios, existen casos en donde el cáncer es detectado en su etapa terminal y aún no se ha encontrado ninguna metodología científica ni empírica que indique la presencia de esta patología. Las metodologías tradicionales de diagnóstico en cualquiera de sus tipos aciertan a un número relativamente bajo de casos en sus etapas tempranas, usando métodos invasivos con el riesgo de ser falsos positivos o falsos negativos. Centros de investigación y universidades han juntado esfuerzos para buscar alternativas de diagnóstico y tratamiento del cáncer usando métodos que permitan mejorar la eficacia del diagnóstico. En este trabajo se aborda un análisis de las diferentes etapas implicadas en el procesamiento de datos de muestras de tejidos cancerosos y saludables usando espectrometría de masas, usando plataformas computacionales, aplicados al mejoramiento de la calidad de las mediciones para posteriores aplicaciones de definición de biomarcadores.Cancer is an asymptomatic disease at an early stage and very difficult to diagnose in many cases it is not perceived until it has already reached the stage of metastasis, spreading in other organs of the body. It is the second cause of death in Ecuador despite progress made in recent years have been revolutionary, there are cases where the cancer is detected in its terminal stage and still has not found any scientific or empirical methodology to indicate the presence of this pathology. Traditional methods of diagnosing cancer in any of its types are relatively ineffective in early stages, using invasive methods and at risk of being detected as false positive or false negative. Research centers and universities have joined forces to seek alternative diagnosis and treatment of cancer using methods to improve the efficiency of diagnosis and detection of cancer in its early stages. This paper discusses a group of algorithms and methodologies for processing data sets of mass spectrometry measurements of cancer and normal analyzed samples using computing platforms aimed at improving the quality of measurements for biomarkers definition applications.Cuenca\n"}
{"prompt":"Centros de procesamientos de datos en la nube ->","completion":" La presente documentación contiene un estudio sobre la Computación en la Nube, que es una tecnología relativamente nueva y que promete grandes bondades a las empresas que buscan reducir sus costos de TI (Tecnologías de Información). Computación en la Nube es una tecnología que va de la mano con la Virtualización, la cual nos ayudará a compartir recursos y optimizar las cargas de trabajo. Nos centraremos de manera más profunda en el estudio de un Centro de Procesamiento de Datos en la Nube, donde analizaremos esta tecnología y la compararemos con los centros de procesamientos de datos tradicionales, a fin de encontrar las virtudes y debilidades de cada una de estas soluciones propuestas. En virtud de brindar robustez al diseño estudiado, se agregará medidas de seguridad al mismo, además de un sistema de recuperación de desastres.This documentation contains a study on computing in the cloud, which is a relatively a new technology that promises great benefits to companies seeking to reduce their IT (information technology) cost. Cloud computing is a technology that goes hand in hand with virtualization, which will help us share resources and optimize workloads. We will focus in a way deeper into the study of a center of data processing in the cloud, where we'll look at this technology and will compare it with the traditional datacenters, in order to find the strengths and weaknesses of each of these proposed solutions. By virtue of providing robustness studied design, security measures will be added to it, as well as a disaster recovery system.\n"}
{"prompt":"Centros de procesamientos de datos en la Nube ->","completion":" La presente documentación contiene un estudio sobre la Computación en la Nube, que es una tecnología relativamente nueva que promete grandes bondades a las empresas que buscan reducir sus costos de TI (Tecnologías de Información). Computación en la Nube es una tecnología que va de la mano con la Virtualización, la cual nos ayudará a compartir recursos y optimizar las cargas de trabajo. Nos centraremos de manera más profunda en el estudio de un Centro de Procesamiento de Datos en la Nube, donde analizaremos esta tecnología y la compararemos con los centros de procesamientos de datos tradicionales, a fin de encontrar las virtudes y debilidades de cada una de estas soluciones propuestas. En virtud de brindar robustez al diseño estudiado, se agregará medidas de seguridad al mismo, además de un sistema de recuperación de desastres.GuayaquilLicenciado en Redes y Sistemas Operativos\n"}
{"prompt":"Análisis de datos meteorológicos del Valle de los Chillos usando datos funcionales ->","completion":" The analysis of functional data (ADF) is a statistical area that has as main objective to study variables whose data vary in a continuous space, and within these variables you can find the time-dependent. Currently you can have a lot of data because most of them the gathering is done automatically, therefore to adjust this data to a function or functional data is one of the characteristics of the functional analysis. These data once converted into functions allow its derivatives to be extracted and explain in a better way how the phenomena under study works. The Meteorological Station Hacienda El Prado, located in the valley of Chillos has systematically analog and automatically registered a variety of meteorological data. With these data what is aimed to do is to model the behavior of the atmospheric variables in that Valley and its area of influence. In this statistical analysis free software R is used, with emphasis on the use of the fda and fda.usc librarys. Its development is based on the exploratory data analysis, selection of the base type, the number of bases, the building of a system based on Fourier series and a model based on the construction of a linear functional operator. In this study two variables have been selected to be studied, they are the daily minimum temperature and daily maximum temperature. The intention of the before mentioned is to give the door open for other studies by selecting other variables, or choosing another weather station and to establish comparisons.El análisis de datos funcionales (ADF) es un área de la estadística que tiene como objetivo principal estudiar variables cuyos datos varíen en un espacio continuo, dentro de ´estas se encuentran las que dependen del tiempo. Actualmente se puede disponer de una gran cantidad de datos debido a que la mayoría de ellos su recolección se hace de manera automática, por lo tanto ajustar ´estos datos a una función o dato funcional es una de las características del análisis funcional, ´estos datos una vez convertidos en funciones permiten que se puedan extraer sus derivadas y explicar de mejora manera los fenómenos en estudio. La Estación Meteorológica de la Hacienda El Prado, ubicada en el valle de los Chillos, ha registrado sistemáticamente y de manera analógica y automática una gran variedad de datos meteorológicos, con ´estos datos lo que se pretende es modelar el comportamiento de las variables atmosféricas en dicho valle y su área de influencia. En ´este análisis se usa software estadístico de libre distribución R, con énfasis en el uso de las librerías fda y fda.usc. Su desarrollo se basara en el análisis exploratorio de datos, la selección del tipo de base, el número de bases, la construcción de un modelo basado en las series de Fourier y un modelo basado en la construcción de un operador funcional lineal. En ´este estudio se ha seleccionado dos variables para estudiar que son las temperaturas diaria mínima máxima. Con esto se pretende dejar una puerta abierta a otros estudios seleccionando otras variables, o escogiendo otra estación meteorológica y poder establecer comparaciones.\n"}
{"prompt":"Análisis, diseño y construcción de un almacén de datos de colocaciones de crédito y captaciones de los bancos privados para aplicar algoritmos de minería de datos ->","completion":" This technical project aims to publicize data mining and applied to private entities for the purpose of discovering information that exists within the data useful, this information is handled through flat files that are publicly accessible, making with the results generated business opportunities in sites not yet overcrowded, increasing revenue and reducing expenditures for organizations. Data mining, you need to study and inquiry into their use and interpretation, also uses algorithmic tools for finding hidden information. This document consists of three chapters, where depth is detailed the relationship and utility proposed to grant proper use and handling of the data mining algorithms. Chapter one describes the tools and materials provided to begin the processing and analysis of information to further streamline the decision-making process. Chapter two describes the scientific and theoretical basis covered with artifacts and terminology used in project implementation. Chapter three describes the implementation of the data obtained and implemented in different data warehouses as well as the results of the study and application of data mining algorithms to predict future behavior in the business of each bank. Finally, they report for bank loans and deposits for them to be manipulated by people interested and are within the decision-making entities are generated.El presente proyecto técnico pretende de dar a conocer la minería de datos y su utilidad aplicada a entidades privadas, con el propósito de descubrir información que existe dentro de la data, esta información es manejada mediante archivos planos que son de acceso público, haciendo que con los resultados obtenidos se generen posibilidades de negocio en sitios aun no abarrotados, incrementando ingresos y reduciendo egresos para las organizaciones. La minería de datos, necesita de estudio y de indagación acerca de su uso e interpretación, así mismo utiliza herramientas algorítmicas para la búsqueda de información oculta. El presente documento consta de tres capítulos, en donde se detalla a profundidad la relación y utilidad propuesta para otorgar adecuado uso y manipulación a los algoritmos de minería de datos. El capítulo uno, describe los instrumentos y materiales proporcionados para comenzar la elaboración y análisis de la información para posteriormente agilizar el proceso de toma de decisiones. El capítulo dos, describe toda la sustentación científica y teórica que cubre con los artefactos y terminología utilizada en la ejecución del proyecto. El capítulo tres, describe la implementación de la data obtenida e implantada en distintos Data Warehouses, así como los resultados obtenidos del estudio y aplicación de algoritmos de minería de datos para predecir futuros comportamientos dentro del negocio de cada entidad bancaria. Por último, se generan reportes para colocaciones y captaciones bancarias para que estos sean manipuladas por personas interesadas y que estén dentro de la toma de decisiones de las entidades.\n"}
{"prompt":"Análisis comparativo entre bases de datos relacionales con bases de datos no relacionales ->","completion":" El documento consiste en realizar un análisis comparativo entre bases de datos relacionales y bases de datos no relacional con respecto a sus funcionalidades, estructuras de almacenamiento, en la determinación de las desventajas del uso de bases de datos relacionales en aplicaciones web no transaccionales, analizar los beneficios de las bases de datos no relacionales frente a las relacionales para aplicaciones web no transaccionales y en la implementación de dos aplicaciones web prototipo tanto para una base de datos relacional como para una base de datos no relacional, estableciendo las condiciones en las que es mejor usar las bases de datos no relacionales en lugar de las bases de datos relacionales.The document consist in a comparative analysis of relational databases and non-relational databases with respect to their functionalities, storage structures, in the determination of the disadvantages of using relational databases in non-transactional web applications, analyzing the benefits of the bases of non-relational data against relational not transactional web applications and implementation of two web application prototype either a relational database as a non-relational databases, establishing the conditions under which it is better using non-relational databases rather than relational databases.\n"}
{"prompt":"Estudio de un sistema de recopilación de datos (caja negra) para buses interprovinciales ->","completion":" En este trabajo se realizó un estudio de un sistema de recopilación de datos \"caja negra\" para buses interprovinciales. El estudio se basó en la adaptación de un sistema ya probado en la industria aeronáutica para determinar accidentes aéreos comúnmente llamada \"caja negra\". Se creó la necesidad de este estudio por la creciente accidentabilidad de buses interprovinciales y para determinar las causas reales de los mismos, se realizó la selección de los periféricos para su monitoreo, modelado digital del prototipo y costos del sistema. Como resultado final se encontró que el proyecto es confiable y con el debido apoyo de las instituciones estatales es ejecutable.\n"}
{"prompt":"Prototipo de robot submarino autónomo tipo torpedo para recopilación de datos oceanográficos. ->","completion":" El presente documento presenta el diseño y construcción de un robot submarino tipo torpedo este equipo está diseñado de tal manera que pueda llegar a desplazarse en el mar con gran rapidez por su diseño hidrodinámico, es utilizado para la recolección de datos oceanográficos, se inicia el diseño 3D del robot con el software Freecad para luego imprimir las piezas más importantes de la estructura del robot y demostrar su flotabilidad e hidrodinámica, la mayoría de los elementos que componen la estructura se encuentran a nivel local como son tubos PVC, tapones PVC y ejes acerados. El diseño realizado posee 2 pares de timones que darán motricidad al robot, el dispositivo encargado de accionarlos son los servomotores que son dispositivos que dependiendo del ancho de pulso enviado se colocaran en posiciones diferentes que van desde los 0° a los 180°, los propulsores son de diseño profesional adquiridos para este trabajo, la parte de control y autonomía del robot está basada en el uso de software y hardware libre. Se diseñó e imprimió un mando inalámbrico para el control del robot para que se utilice cuando esté en la superficie, el robot submarino y el mando inalámbrico tendrán un alcance de máximo 200 metros, en la PCB de la tarjeta de control se realizó una etapa para cargar inalámbricamente un firmware, este proceso se lo realiza para evitar desmontar la parte electrónica de los recipientes herméticos cada vez que se necesite hacer algún cambio en el firmware de la tarjeta principal, además el mando inalámbrico se comunica con la aplicación móvil para presentar en mapas virtuales la trayectoria recorrida por el robot. El robot tiene la capacidad de desplazarse utilizando el método de recorrido deducido y en su interior posee un sensor lm35 que ha sido hermetizado en la parte de los pines para evitar que tengan contactos con el agua, con la finalidad de recolectar datos oceanográficos como temperatura y mostrar en la aplicación.\n"}
{"prompt":"Sistema automático georreferenciado para recopilación de datos técnicos en estaciones Base ->","completion":" El presente proyecto de grado comprende el estudio, diseño e implementación de un sistema automático que permite controlar el trabajo realizado por un proveedor de mantenimiento externo a las estaciones base de Telefónica-Movistar Ecuador, usando para ello un sistema integral georeferenciado, el cual está constituido por la aplicación móvil que se encuentra en los equipos celulares de los técnicos de campo, conectividad de los mismos con los servidores de La Compañía, y la presentación de datos en forma de reportes ejecutivos a través de la página web de Telefónica. Mediante la georeferencia incluida en la recopilación de datos en campo, se pudo determinar si los reportes se estaban completando desde las estaciones correspondientes así como en la fecha planificada, esto, mediante la comparación con una tabla de coordenadas geográficas pre-cargadas de las estaciones base en el servidor de intranet de La Compañía.\n"}
{"prompt":"Propuesta de una metodología para la evaluación de métodos de preprocesamiento de trayectorias GPS. ->","completion":" PDFEl presente trabajo de titulación plantea la creación de una metodología generalizada que permita la evaluación de métodos utilizados en el área del preprocesamiento de trayectorias GPS, para ello se plantea la clasificación de trayectorias de manera generalizada en base a la identificación de sus características que pueda abarcar una gran variedad de estas. El desarrollo de la investigación se basa en la exploración de diversos métodos utilizados para el preprocesamiento de trayectorias, que se utilizan para realizar una clasificación en etapas de acuerdo a la aplicabilidad la cuales fueron evaluadas mediante experimentación, utilizando cinco diferentes conjuntos de datos. Para validación de la propuesta se aplicó un juicio de expertos y un análisis mediante estadística descriptiva para identificar si la metodología propuesta es factible, al final los resultados han sido favorablesThis degree work proposes the creation of a generalized methodology that allows the evaluation of methods used in the area of GPS trajectory preprocessing, for it is proposed the classification of trajectories in a generalized manner based on the identification of their characteristics that can cover a wide variety of these. The development of the research is based on the exploration of various methods used for the preprocessing of trajectories, which are used to perform a classification in stages according to the applicability which were evaluated by experimentation, using five different data sets. In order to validate the proposal, an expert judgment and an analysis by descriptive statistics were applied to identify if the proposed methodology is feasible, at the end the results have been favorable\n"}
{"prompt":"Aplicación de La Red Neuronal Artificial Feedforward Backpropagation para la predicción de demanda de energía eléctrica en la Empresa Eléctrica Riobamba S.A. ->","completion":" Esta investigación propone un modelo basado en la red neuronal Artificial Feedforward Back propagation capaz de predecir la demanda de energía eléctrica con un porcentaje de error absoluto inferior al generado por la metodología utilizada por una distribuidora, con lo cual se pretende contribuir con la planificación de operación y mantenimiento de las centrales eléctricas y a su vez servir de modelo para otras instituciones con similares características. Se realizó la observación de campo a las subestaciones y medidores de las tres salidas, donde se obtuvieron 70128 observaciones de los cuales 61344 se utilizó para el entrenamiento de la red y 8784 para las pruebas del modelo. Mediante preprocesamiento de datos se detectó y corrigió 406 datos perdidos y 320 datos atípicos, mismos que en su mayoría corresponden al año 2009 y 2014, se determinó que el porcentaje del error medio absoluto (MAPE) del modelo de predicción de la demanda de energía eléctrica basado en la red neuronal FeedForward Backpropagation fue del 2,63%, mientras que el basado en regresión lineal múltiple fue del 4,56%. Se concluye que el modelo de predicción de la demanda de energía eléctrica basado en la red neuronal FeedForward Backpropagation es tiene mejor rendimiento de predicción. Se recomienda antes de diseñar un modelo neuronal realizar el pre-procesamiento de los datos para corregir datos atípicos, perdidos y suavizado de la serie temporal con el fin de obtener resultados satisfactorios.This research proposes a model based on the artificial neural network Feedforward Back propagation capable of predicting the demand of electric power with a percentage of absolute error lower than the one generated due to the methodology used by a distributor, with which it is intended to contribute to the planning of operation and maintenance of power plants and at the same time to serve as a model for other institutions with similar characteristics. Field observations was performed on the substations and measurers of the three outputs where 70128 observations were obtained, of which 61344 were used for the training of the network and 8784 for tests of the model. Through pre-processing of data it was detected and corrected 406 lost data and 320 atypical data, which the majority correspond to the year 2009 and 2014, it was determined that the percentage of mean absolute error (MAPE) of the prediction model of the demand electrical energy based on the neural network Feedforward Back propagation was 2.63%, while the one based on multiple linear regression was 4.56%. It is concluded that the prediction model of the demand for electrical energy based on the neural network FeedForward Backpropagation holds better prediction performance. Before designing a neural, it is recommended to perform the preprocessing of data to correct atypical data, lost and softened of the time series in order to obtain satisfactory results.\n"}
{"prompt":"Modelo de predicción de la calidad del aire a partir de datos meteorológicos e información del tráfico automovilístico ->","completion":" Air pollution poses a major environmental risk to health. Only by seeking to reduce the levels of this pollution, countries can reduce the burden of disease resulting from stroke, lung cancers and chronic and acute pneumopathies, including asthma. (World Health Organization, 2016). In the article Modeling PM2.5 Urban Pollution Using Machine Learning and Selected Meteorological Parameters published on June 18,2017 by Yves Rybarczyk, Mario Gonzalez and Rasa Zalakeiciute, professors at the University of the Americas; they propose an automatic learning approach based on six years of analysis of meteorological data and pollution in order to predict concentrations of fine particulate matter (PM2).5) from wind levels (speed and direction) and precipitation by applying the Linear Regression algorithm. In this thesis document, Machine Learning concepts are defined as: Supervised Learning, Classifiers, Regression, among other topics. In order to obtain the information, two applications were executed in data collection, which were obtained by the following methods: traffic time data managed by Google Maps. Screens captured from Google Maps, obtaining traffic information represented in red, orange and green, graphically rectangular shape. And finally, data was collected by making a circular area cut. All the information obtained in these three methods was stored in a CSV file, which is refreshed every 10 minutes. Once the CSV files had been obtained, the data preprocessing was carried out, cleaning the information by filling the empty spaces of the CSV files with the ? sign. To load them into the Weka software, we worked with Linear Regression, classifiers such as Neural Networks, SVM, Logistic Regression and KNN. Using the results obtained, confusion matrices were developed for the construction of the ROC curve in order to obtain the best performing supervised learning technique. In the future, an application can be developed with the aim of making this entity aware of the level of contamination to which the population is exposed and taking corrective measures.La contaminación del aire representa un importante riesgo medioambiental para la salud. Únicamente buscando disminuir los niveles de esta contaminación, los países pueden reducir la carga de morbilidad derivada de accidentes cerebrovasculares, cánceres de pulmón y neumopatías crónicas y agudas, entre ellas el asma. (Organización Mundial de la Salud, 2016). En el artículo Modeling PM2.5 Urban Pollution Using Machine Learning and Selected Meteorological Parameters publicado el 18 de Junio de 2017 por Yves Rybarczyk, Mario Gonzalez y Rasa Zalakeiciute, docentes de la Universidad de las Américas; proponen un enfoque de aprendizaje automático basado en seis años de análisis de datos meteorológicos y de contaminación con el fin de predecir las concentraciones de material particulado fino (PM2.5) a partir de los niveles de viento (velocidad y dirección) y precipitación aplicando el algoritmo de Regresión Lineal. En el presente documento de tesis, se definen conceptos de Machine Learning como: Aprendizaje Supervisado, Clasificadores, Regresión, entre otros temas. Para obtener la información, se ejecutaron dos aplicaciones en la recolección de datos, los cuales se obtuvieron por los métodos que a continuación se detallan: datos del tiempo de tráfico administrado por Google Maps. Pantallas capturadas de Google Maps, obteniendo información del tráfico representado en colores rojo, naranja y verde, graficados en forma rectangular. Y, por último, se recolectaron datos realizando un corte de área circular. Toda la información obtenida en estos tres métodos se almacenó en un archivo CSV, el mismo que se refresca cada 10 minutos. Ya obtenidos los archivos CSV, se realizó el preprocesamiento de datos procediendo a una limpieza de la información llenando los espacios vacíos de los archivos CSV con el signo ? para cargarlos en el software Weka, se trabajó con Regresión Lineal, los clasificadores como Redes Neuronales, SVM, Regresión Logística y KNN. Con los resultados obtenidos se elaboraron matrices de confusión para la construcción de la curva ROC a fin de obtener la técnica de aprendizaje supervisado con mejor desempeño. A futuro se podrá desarrollar una aplicación, con el objetivo de que este ente conozca el nivel de contaminación al que la población se expone y tomar medidas correctivas.\n"}
{"prompt":"Evaluación de técnicas de procesamiento de lenguaje natural para textos cortos en lenguaje español ->","completion":" El Procesamiento de Lenguaje Natural identifica información clave, generando modelos predictivos y explicando eventos o tendencias mundiales, creando conocimiento, por ello, es importante aplicar técnicas de refinamiento en etapas principales como el pre-procesamiento, al producirse con frecuencia datos y procesamiento con resultados deficientes. Este documento analiza y mide el impacto de combinaciones de técnicas y librerías para textos cortos en español, mediante su aplicación en tweets aplicados al análisis de sentimiento, tomando en cuenta parámetros de evaluación en su análisis, el tiempo de procesamiento y características de las técnicas en cada librería. La experimentación demostró que elegir combinaciones de técnicas adecuadas en el pre-procesamiento, proporciona una mejora de hasta un 5% y 9% en el rendimiento de la clasificación.Ingeniero en Sistemas y Telemática\n"}
{"prompt":"Minería de opinión para textos en español usando procesamiento natural del lenguaje ->","completion":" Este artículo se propone el desarrollo de una herramienta que permite la minería de opiniones de textos exclusivamente en español. Para ello, se realizó la construcción de un corpus lingüístico a través de tweets. Este corpus se caracteriza por tener tweets en idioma español latinoamericano y español castellano. Los algoritmos de clasificación usados son: Naïve Bayes, SVM y LSTM. Para la evaluación de la propuesta se realizaron experimentos con un corpus de 14.666 con una clasificación en dos clases (positivo y negativo), luego en tres clases (positivo, negativo, neutro). Los Resultados obtenidos muestran que la clasificación binaria obtuvo mejores resultados que en la clasificación a tres clases en los tres algoritmos.This paper proposes development of a tool that allows opinion mining of texts exclusively in Spanish. For this, a linguistic corpus has been constructed through tweets. This corpus is characterized by having tweets in Latin American Spanish and Castilian Spanish. The evaluated classification algorithms were: Naive Bayes, SVM, and LSTM. The experiments were carried out with a corpus of 14,666 with a classification in two classes (positive and negative) in three classes (positive, negative, neutral). The results obtained show that the binary classification got better results than the three-class classification in the three algorithms.\n"}
{"prompt":"Clasificación de documentos científicos mediante técnicas de procesamiento de lenguaje natural y minería de texto ->","completion":" Abstract:The Universidad Técnica Particular de Loja, , with the aim of promoting scientific research, creates groups of research lines to create, socialize research and disseminate in several scientific databases. The articles that are included in the different lines. This degree work aims to determine the relationships between the research lines and the terms of the articles uploaded to SCOPUS from 2003 to 2017; through the collection of information, elaboration of vocabulary, supervised classification, preprocessing and data training. The methodology is the \"metametodología\", composed of four principles that allow to obtain the result of the proposed research: obtain the result of 623 documents in plain text; Information on the abstract, the author and the keywords of each article was compiled, and a new classification was made due to inconsistencies in the classification. The application of the nearest k algorithms (KNN) and linear discriminant analysis (LDA) shows the accuracy of the classification of the articles, as well as the relationship that exists between them.\n"}
{"prompt":"Desarrollo de un envoltorio del api-rest de mendeley con Graphql ->","completion":" Desarrollar un envoltorio del API-REST del gestor bibliográfico Mendeley utilizando el lenguaje de consultas GraphQL y validado con un marco de trabajo de calidad en uso basado en el estándar ISO\/IEC 25000La creación de sistemas ha experimentado cambios en su proceso de construcción, la separación del servidor y cliente ha impulsado la innovación tecnológica por medio de la creación de tecnologías más útiles y eficientes. La integración entre el cliente y servidor normalmente es realizada por una API, comúnmente usando tecnología REST la cual es muy aceptada en la comunidad de desarrollo de software, aunque es muy aceptada también presenta inconvenientes como la complejidad que presenta su consumo lo que hace necesario la creación y utilización de alternativas, una de estas alternativas es GraphQL que es un lenguaje de consultas para las APIs y que pretende ser más eficiente y mejorar circunstancialmente la experiencia de desarrollo del lado del cliente. El presente trabajo describe la creación de una API que envuelve la tecnología API-REST y la convierte en tecnología GraphQL (envoltorio), se tomó a la API-REST de Mendeley como caso de desarrollo. Para la creación del envoltorio se definió una base teórica que permitió conocer conceptos que involucra crear el envoltorio. El desarrollo del producto se lo hizo siguiendo la metodología ágil Scrum que comprende ciclos interactivos llamados Sprints. Para validar la usabilidad del software, se usó un taller práctico y una encuesta que fueron validadas estadísticamente y que permitieron recolectar datos los cuales fueron usados para evaluar la calidad en uso con un marco de trabajo basado en el estándar ISO\/IEC 25000, por lo cual se obtuvo resultados satisfactorios.Ingeniería\n"}
{"prompt":"Implementación de una solución e-commerce en ambiente móvil con flutter usando los servicios Rest Api de Woocomerce. ->","completion":" PDFActualmente por la pandemia que atraviesa el mundo, ocasionada por el COVID-19 muchos negocios se han visto en la obligación de ofrecer y realizar sus transacciones de ventas por medio de medios digitales o crear sus propios aplicativos móviles para llevar sus productos a mayor cantidad de consumidores ya que 53% de compras en líneas son realizadas por medio de dispositivos móviles . El presente trabajo busca el desarrollo de una aplicación móvil con Flutter , el cual es un Framework de código abierto desarrollado por Google que ofrece la creación de un solo proyecto para iOS y Android ejecutandose como auténticas aplicaciones nativas en los dispositivos, la aplicación hára uso de los servicios Rest API de WooCommerce para el manejo de una tienda electrónica, que es una plataforma en la que mezcla la destreza de un módulo de e-commerce altamente personalizable y a su vez eficiente en un ecosistema experimentado como lo es WordPress, administrador de contenidos con módulos, extensiones y plugins que pueden funcionar en cualquier aplicación web. Existen aplicaciones que incorporan los servicios Rest pero en su mayoría no son de libre acceso o no cumple con las características requeridas por los usuarios. Por medio de la metodología de investigación se podrá recolectar información de múltiples fuentes bibliográficas que serán de apoyo para el correcto desarrollo y funcionamiento de la aplicación, así mismo el desarrollo de esta aplicación permitirá la portabilidad y acceso a los consumidores de las tiendas online para así brindar oportunidades a las PYMES de la ciudad de Guayaquil de tener una solución E-commerce desde sus dispositivos Android o iOS ya que siendo Open Source y multiplataforma tendrán una oportunidad de llegar a más usuarios de forma gratuita.Currently, due to the pandemic that is going through the world, caused by COVID -19, many businesses have been forced to offer and carry out their sales transactions through digi tal means or create their own mobile applications to take their products to a greater number of consumers since 53% of online purchases are made through mobile devices. The present work seeks the development of a mobile application with Flutter which is an open source Framework developed by Google that offers the creation of a single project for iOS and Android running as authentic native applications on devices, the application will use the WooCommerce Rest API services for managing an electronic store, wh ich is a platform that mixes the skills of a highly customizable and efficient e-commerce module in an experienced ecosystem such as WordPress, content manager with modules, extensions and plugins that can work in any web application. There are applications that incorporate Rest services, but most of them are not freely accessible or do not meet the characteristics required by users. Through the research methodology, information can be collected from multiple bibliographic sources that will be of support fo r the correct development and operation of the application, likewise the development of this application will allow the portability and access to consumers of online stores in order to provide opportunities to SMEs in the city of Guayaquil to have an E -commerce solution from their Android or iOS devices since being Open Source and multiplatform they will have an opportunity to reach more users for free.\n"}
{"prompt":"Análisis de frameworks de desarrollo de api rest y su impacto en el rendimiento de aplicaciones web con arquitectura Spa ->","completion":" Analizar el impacto del WBAF (Web Backend Application Framework) de API REST en el rendimiento de aplicaciones web dinámicas con SPA (Single Page Application) para definir el WBAF que mejor se adapte al desarrollo de aplicaciones web del Gobierno Provincial de Imbabura.Para decidir el WBAF (Web Backend Application Framework) para el desarrollo de API REST, que mejor se ajuste a las necesidades y arquitectura de una aplicación web, es importante referirse al rendimiento de las aplicaciones. La alta demanda de una mejor funcionalidad y usabilidad en aplicaciones web puso en funcionamiento nuevas técnicas, con la arquitectura SPA utiliza la técnica de buscar datos asincrónicamente y actualizar el contenido de la página o el componente entero sin actualizarlo. El frontend debe tener un bajo acoplamiento a las API REST desarrolladas en el backend; en esta investigación planteamos si el framework de desarrollo de las API REST impacta en el rendimiento de la aplicación web con arquitectura SPA. Los WBAF analizados son: Express.js que es un web application framework para Node.js y el framework ASP.NET Core. Mediante el Mapping Study se determinó que las métricas Response Time y Throughput determinan el rendimiento de las aplicaciones web y la metodología de benchmark nos sirve para obtener información sobre el comportamiento del rendimiento de las API REST, con los frameworks Express.js y ASP.NET Core para posteriormente compararlos con respecto al rendimiento de la aplicación web. Como conclusión podemos decir que si impacta el framework de desarrollo de API REST sobre el rendimiento de ejecución de aplicaciones web con SPA solamente cuando es una cantidad elevada de usuarios concurrentes, caso contrario ante la percepción del usuario no impactaría el framework de desarrollo.\n"}
